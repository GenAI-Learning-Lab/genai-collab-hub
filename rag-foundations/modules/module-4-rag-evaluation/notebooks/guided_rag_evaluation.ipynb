{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ49uOGGSYOb"
      },
      "source": [
        "### **Module 4: Measuring Performance with RAGAs**\n",
        "\n",
        "**Objective:**\n",
        "In our previous modules, we've focused on improving our RAG pipeline's performance based on our intuition. Now, we will introduce a **quantitative and automated** way to measure its quality. The objective of this module is to build an evaluation pipeline using the industry-standard **RAGAs** framework to score our system on key metrics like factual consistency and relevance.\n",
        "\n",
        "**Core Concept: RAG Evaluation**\n",
        "You can't improve what you can't measure. A RAG evaluation framework allows us to move beyond \"it feels better\" to a data-driven approach. It works by taking a small, curated set of questions and \"golden\" answers (ground truth) and using them to score our pipeline's performance. RAGAs cleverly uses powerful LLMs as judges to assess the quality of the retrieved context and the generated answer, providing us with a \"report card\" for our system.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this module, you will be able to:\n",
        "\n",
        "  * Understand the importance of quantitative evaluation for RAG systems.\n",
        "  * Explain the core RAGAs metrics: **Faithfulness**, **Answer Relevancy**, **Context Precision**, and **Context Recall**.\n",
        "  * Prepare a test dataset in the format required by RAGAs, including creating \"ground truth\" answers.\n",
        "  * Execute an evaluation pipeline using RAGAs to score the advanced system we built in Module 3.\n",
        "  * Analyze the RAGAs scores to identify the specific strengths and weaknesses of the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIoaWOvaSYOb"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 1: Install Dependencies & Import Libraries**\n",
        "\n",
        "First, let's get our environment ready by installing the necessary Python packages and importing all the modules we'll need for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSz7Ne91SYOc"
      },
      "outputs": [],
      "source": [
        "!pip install -q ragas datasets\n",
        "!pip install -q langchain langchain-community langchain-groq qdrant-client pypdf fastembed langchain_huggingface sentence_transformers\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# LangChain and related imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Qdrant, Reranking, and Sparse Embedding imports\n",
        "from qdrant_client import QdrantClient, models\n",
        "from fastembed import SparseTextEmbedding\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "# RAGAs evaluation imports\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "print(\"All libraries installed and imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4giB0a3SYOc"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 2: Configure API Key and Load Data**\n",
        "\n",
        "Here, we'll set up our Groq API key and load the NVIDIA financial report PDF. This part is provided for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUpijwkwSYOc"
      },
      "outputs": [],
      "source": [
        "# Ensure you have 'GROQ_API_KEY' saved as a secret in your Colab environment\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Ensure the PDF file is uploaded to your Colab session\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "if not os.path.exists(pdf_path):\n",
        "    raise FileNotFoundError(f\"The file {pdf_path} was not found. Please upload it to the Colab environment.\")\n",
        "\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1YZWRAKSYOc"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 3: Initialize RAG Components**\n",
        "\n",
        "> **Your Task:** Initialize all the core components for our RAG pipeline. This includes:\n",
        "> 1.  The **Qdrant** client and a new collection named `\"rag_foundations_m4\"` with both `dense` and `sparse` vector configurations.\n",
        "> 2.  The **dense embedding model** (`BAAI/bge-m3`).\n",
        "> 3.  The **sparse embedding model** (`prithivida/Splade_PP_en_v1`).\n",
        "> 4.  The **cross-encoder model** for re-ranking (`BAAI/bge-reranker-base`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kyuZVrBSYOc"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize Qdrant Client\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "collection_name = \"rag_foundations_m4\"\n",
        "\n",
        "# 2. Create the collection with dense and sparse vector support\n",
        "# YOUR CODE HERE\n",
        "client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config={\"dense\": models.VectorParams(size=1024, distance=models.Distance.COSINE)},\n",
        "    sparse_vectors_config={\"text-sparse\": models.SparseVectorParams(index=models.SparseIndexParams(on_disk=False))}\n",
        ")\n",
        "\n",
        "# 3. Initialize the dense embedding model\n",
        "# YOUR CODE HERE\n",
        "dense_embed_model = ...\n",
        "\n",
        "# 4. Initialize the sparse embedding model\n",
        "# YOUR CODE HERE\n",
        "sparse_embed_model = ...\n",
        "\n",
        "# 5. Initialize the reranker model\n",
        "# YOUR CODE HERE\n",
        "cross_encoder = ...\n",
        "\n",
        "print(\"Models and Qdrant collection initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d81Vw5lSYOd"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 4: Embed and Upsert Documents**\n",
        "\n",
        "> **Your Task:** Loop through all the document chunks (`docs`). For each chunk, generate its dense and sparse vector embeddings and create a Qdrant `PointStruct` to be upserted. Collect all these points and perform a single bulk `upsert` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIBD4QkSSYOd"
      },
      "outputs": [],
      "source": [
        "points_to_upsert = []\n",
        "for i, doc in enumerate(tqdm(docs, desc=\"Upserting documents\")):\n",
        "    # YOUR CODE HERE: Generate dense and sparse vectors\n",
        "    dense_vec = ...\n",
        "    sparse_vec = ...\n",
        "\n",
        "    # YOUR CODE HERE: Create a Qdrant PointStruct\n",
        "    # Remember to include payload and the two vector types.\n",
        "    points_to_upsert.append(models.PointStruct(\n",
        "        id=i,\n",
        "        payload=...,\n",
        "        vector={...}\n",
        "    ))\n",
        "\n",
        "# YOUR CODE HERE: Upsert the points to the Qdrant collection\n",
        "client.upsert(collection_name=..., points=..., wait=True)\n",
        "print(f\"Upserted all {len(docs)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q_kRswYSYOd"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 5: Build the RAG Chain**\n",
        "\n",
        "> **Your Task:** This is a crucial step. You need to build an efficient RAG chain that performs retrieval, re-ranking, and generation in a single call.\n",
        ">\n",
        "> 1.  Initialize the LLM (`meta-llama/llama-4-scout-17b-16e-instruct` via Groq).\n",
        "> 2.  Complete the `rerank_and_retrieve` function to perform hybrid search and reranking.\n",
        "> 3.  Define the `prompt` using the provided template string.\n",
        "> 4.  Construct the final `rag_chain` using LCEL. This chain must take a dictionary `{\"question\": \"...\"}` as input and produce a dictionary `{\"answer\": \"...\", \"contexts\": [...]}` as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqTT5myaSYOd"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE: Initialize the LLM\n",
        "llm = ...\n",
        "\n",
        "def rerank_and_retrieve(query: str):\n",
        "    \"\"\"\n",
        "    Performs hybrid search (dense + sparse) and then re-ranks the results.\n",
        "    \"\"\"\n",
        "    top_k_retrieval = 10\n",
        "    query_with_prefix = f\"query: {query}\"\n",
        "\n",
        "    # YOUR CODE HERE: Generate dense and sparse query vectors\n",
        "    dense_query_vec = ...\n",
        "    sparse_query_vec = ...\n",
        "\n",
        "    # YOUR CODE HERE: Perform dense and sparse search against Qdrant\n",
        "    dense_results = client.search(...)\n",
        "    sparse_results = client.search(...)\n",
        "\n",
        "    # Combine and de-duplicate results (this part is provided)\n",
        "    seen_ids = set()\n",
        "    candidate_docs = []\n",
        "    for result in dense_results + sparse_results:\n",
        "        if result.id not in seen_ids:\n",
        "            candidate_docs.append(result.payload['text'])\n",
        "            seen_ids.add(result.id)\n",
        "\n",
        "    # YOUR CODE HERE: Rerank the results using the cross_encoder\n",
        "    rerank_pairs = ...\n",
        "    rerank_scores = ...\n",
        "    doc_with_scores = list(zip(candidate_docs, rerank_scores))\n",
        "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the top_k documents after re-ranking\n",
        "    top_k_rerank = 3\n",
        "    return [doc[0] for doc in sorted_docs[:top_k_rerank]]\n",
        "\n",
        "# YOUR CODE HERE: Define the prompt template\n",
        "prompt_template = \"\"\"...\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "retriever = RunnableLambda(rerank_and_retrieve)\n",
        "\n",
        "# YOUR CODE HERE: Construct the efficient RAG chain using LCEL\n",
        "# Hint: Use RunnablePassthrough.assign(...) to add keys to the data as it flows through the chain.\n",
        "rag_chain = (\n",
        "    ...\n",
        ")\n",
        "\n",
        "print(\"--- RAG chain created ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsRdwLIRSYOe"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 6: Create the Evaluation Dataset**\n",
        "\n",
        "\n",
        "> **This Task is already completed for you:**\n",
        "> 1.  Define the test `questions` and their corresponding `ground_truths` answers.\n",
        "> 2.  Loop through the questions, `invoke` the chain you just built, and collect the resulting `answers` and `contexts`.\n",
        "> 3.  Create the final `ragas_dataset` using `Dataset.from_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJyf5zZDSYOe"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE: Define the questions and ground_truths lists\n",
        "questions = [\n",
        "    \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\",\n",
        "    \"What was the exact value for 'Tax withholding related to common stock from stock plans' for the period ending April 27, 2025?\",\n",
        "    \"What specific action did the U.S. government take on April 9, 2025, that impacted H20 products?\"\n",
        "]\n",
        "\n",
        "ground_truths = [\n",
        "    \"During the first quarter of fiscal year 2026, NVIDIA repurchased 126 million shares of its common stock for $14.5 billion.\",\n",
        "    \"The exact value for tax withholding related to common stock from stock plans for the period ending April 27, 2025 (Q1 FY26) was $1,532 million.\",\n",
        "    \"On April 9, 2025, the U.S. government informed NVIDIA that it requires a license for the export of its H20 integrated circuits to China.\",\n",
        "]\n",
        "\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for query in tqdm(questions, desc=\"Generating answers and contexts\"):\n",
        "    result = rag_chain.invoke({\"question\": query})\n",
        "    answers.append(result[\"answer\"])\n",
        "    contexts.append(result[\"contexts\"])\n",
        "\n",
        "ragas_dataset = Dataset.from_dict({\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truth\": ground_truths\n",
        "})\n",
        "\n",
        "print(\"\\nEvaluation dataset created successfully.\")\n",
        "print(ragas_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzYYpO7-SYOe"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 7: Configure and Run the RAGAs Evaluation**\n",
        "\n",
        "> **Your Task:**\n",
        "> 1. Define the list of `metrics` you want RAGAs to compute.\n",
        "> 2. Call the `evaluate` function from RAGAs, passing the `ragas_dataset`, the `metrics` list, the `llm` object, and the `dense_embed_model` object.\n",
        "> 3. Display the final results as a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HeB8sxGSYOe"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE: Define the list of metrics\n",
        "metrics = [\n",
        "    ...\n",
        "]\n",
        "\n",
        "async def run_evaluation():\n",
        "    print(\"Running RAGAs evaluation...\")\n",
        "    # YOUR CODE HERE: Call the RAGAs evaluate function\n",
        "    result = evaluate(\n",
        "        dataset=...,\n",
        "        metrics=...,\n",
        "        llm=...,\n",
        "        embeddings=...\n",
        "    )\n",
        "    print(\"Evaluation complete.\")\n",
        "    return result\n",
        "\n",
        "# Run the async evaluation function\n",
        "result = asyncio.run(run_evaluation())\n",
        "\n",
        "# YOUR CODE HERE: Convert the result to a pandas DataFrame and display it\n",
        "df = ...\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Gpe1y_SYOe"
      },
      "source": [
        "---\n",
        "### **Module 4: Conclusion & Analysis**\n",
        "\n",
        "After running the evaluation, you will have a \"report card\" for your RAG system.\n",
        "\n",
        "**How to Interpret the Scores:**\n",
        "\n",
        "  * **`faithfulness`:** This is the most important metric. It checks if the answer is factually consistent with the provided context. A score of 1.0 is perfect; a score of 0 means the answer is completely made up.\n",
        "  * **`answer_relevancy`:** This measures how well the answer addresses the actual question. It ignores factual accuracy and just focuses on whether the answer is \"on-topic.\"\n",
        "  * **`context_precision`:** This scores the retriever. It asks: \"Of the context we provided, how much of it was actually useful?\" A high score means we are not passing a lot of \"noise\" to the LLM.\n",
        "  * **`context_recall`:** This also scores the retriever. It asks: \"Did we find all the necessary information needed to answer the question?\" A high score means our retriever didn't miss any critical information.\n",
        "\n",
        "By analyzing these scores, you can now scientifically prove the quality of your RAG system and diagnose where it needs improvement. For example, if `context_recall` is low, you need to improve your retriever. If `faithfulness` is low, you may need to improve your prompt or use a better generation model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}