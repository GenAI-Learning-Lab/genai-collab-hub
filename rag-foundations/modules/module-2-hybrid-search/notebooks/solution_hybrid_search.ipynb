{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Solution Notebook: Module 2 - Hybrid Search**\n",
        "\n",
        "*This notebook contains the solutions for the guided hands-on exercise.*\n",
        "\n",
        "-----\n",
        "\n",
        "### **Module 2: Improving Recall with Hybrid Search**\n",
        "\n",
        "**Objective:**\n",
        "In our first module, we saw a critical **Recall Failure**. Our basic RAG system, using only semantic search, completely missed the correct document chunk for a query about \"share repurchases.\" It failed to find the right information in the knowledge base.\n",
        "\n",
        "The objective of this module is to solve that recall problem by implementing a more powerful **Hybrid Search** system. We will combine traditional keyword-based search with the semantic search we've already learned. This will create a much more reliable retriever.\n",
        "\n",
        "**Learning Objectives:**\n",
        "By the end of this module, you will be able to:\n",
        "- Explain the core concept of Hybrid Search and understand the distinct roles of dense (semantic) and sparse (keyword) vectors.\n",
        "- Implement a hybrid data strategy by creating both dense and sparse embeddings for your documents using open-source models.\n",
        "- Configure and populate a Qdrant collection that handles a sophisticated hybrid search workload.\n",
        "- Build a custom retrieval function that performs both dense and sparse searches and fuses the results.\n",
        "- Diagnose a **Recall Failure** and understand why a narrow search (`k=4`) can cause the system to fail, even with a better algorithm.\n",
        "\n",
        "**Core Concept: Hybrid Search with Qdrant**\n",
        "We will create and store two types of vectors for each document chunk:\n",
        "1.  **Dense Vector (from `bge-m3`):** Captures the *semantic meaning* and conceptual relationships.\n",
        "2.  **Sparse Vector (from `Splade`):** Captures the *keyword importance*.\n",
        "\n",
        "When a query comes in, our system will perform two separate searches—one for meaning and one for keywords—and then combine the results. This gives us the best of both worlds, making our system far more robust against the type of keyword-based failure we saw in Module 1.\n"
      ],
      "metadata": {
        "id": "QaVFG2yHMfxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Install Dependencies**"
      ],
      "metadata": {
        "id": "CyQKsdC9Q06f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries\n",
        "!pip install -q langchain langchain-community langchain-groq langchain_huggingface qdrant-client pypdf fastembed\n",
        "\n",
        "# Ignore standard warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGTx86A0MfxY",
        "outputId": "69ea93f5-f827-4148-f8a1-4ea089f514d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 2: Setup API Key & Document Loading**\n",
        "\n",
        "\n",
        "\n",
        "This step remains the same as Module 1. In this module, we reuse our Module-1 API keys, we load the NVIDIA financial report PDF, and split it into chunks."
      ],
      "metadata": {
        "id": "AC_txeIoMfxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Setup API Key ---\n",
        "# Make sure you have added your GROQ_API_KEY to the Colab secrets manager\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# --- Load and Split Document ---\n",
        "# Make sure you have uploaded the NVIDIA Q1 FY26 PDF to your Colab session\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Use the same chunking strategy as Module 1\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document loaded and split into 191 chunks.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Q9cJMMMfxa",
        "outputId": "d2980a22-8baf-4819-c372-7a121942366c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 3: Initialize Qdrant for Hybrid Search**\n",
        "\n",
        "This is a key step. We will create a Qdrant client and then create a new **collection** that is specifically configured to handle both dense and sparse vectors. This is different from Module 1 where we only had one type of vector.\n"
      ],
      "metadata": {
        "id": "dZMs6FiCMfxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "# Initialize an in-memory Qdrant client\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"rag_foundations_m2_guided\"\n",
        "\n",
        "# Create the collection with configurations for both dense and sparse vectors\n",
        "print(f\"Creating Qdrant collection '{collection_name}' for hybrid search...\")\n",
        "\n",
        "# SOLUTION\n",
        "client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config={\n",
        "        \"dense\": models.VectorParams(size=1024, distance=models.Distance.COSINE)\n",
        "    },\n",
        "    sparse_vectors_config={\n",
        "        \"text-sparse\": models.SparseVectorParams(\n",
        "            index=models.SparseIndexParams(\n",
        "                on_disk=False\n",
        "            )\n",
        "        )\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Collection created successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Qdrant collection 'rag_foundations_m2_guided' for hybrid search...\n",
            "Collection created successfully.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0cfxsiUMfxa",
        "outputId": "08017db2-76d6-4aad-ba6e-8bec0be988e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 4: Embed and Store Documents**\n",
        "\n",
        "\n",
        "Now we will perform the main data processing. We will loop through every document chunk, create both a dense and a sparse vector for it, and then store them together in our new Qdrant collection."
      ],
      "metadata": {
        "id": "whFRJtskMfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "#from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "from fastembed import SparseTextEmbedding\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Initializing local embedding models...\")\n",
        "# 1. Initialize our embedding models\n",
        "dense_embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "sparse_embed_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
        "print(\"Models initialized.\")\n",
        "\n",
        "# 2. Embed and prepare all documents for upsert\n",
        "print(\"Embedding and preparing all documents for upsert...\")\n",
        "points_to_upsert = []\n",
        "for i, doc in enumerate(tqdm(docs, desc=\"Processing All Documents\")):\n",
        "    doc_text = doc.page_content\n",
        "\n",
        "    # SOLUTION (Part 1)\n",
        "    # Create the dense vector for the doc_text.\n",
        "    dense_vec = dense_embed_model.embed_query(doc_text)\n",
        "\n",
        "    # SOLUTION (Part 2)\n",
        "    # Create the sparse vector for the doc_text.\n",
        "    sparse_vec = list(sparse_embed_model.embed([doc_text]))[0]\n",
        "\n",
        "    # SOLUTION (Part 3)\n",
        "    # Create a Qdrant PointStruct to hold all the data.\n",
        "    point = models.PointStruct(\n",
        "        id=i,\n",
        "        payload={\"text\": doc_text, **doc.metadata},\n",
        "        vector={\n",
        "            \"dense\": dense_vec,\n",
        "            \"text-sparse\": models.SparseVector(\n",
        "                indices=sparse_vec.indices.tolist(),\n",
        "                values=sparse_vec.values.tolist()\n",
        "            ),\n",
        "        },\n",
        "    )\n",
        "\n",
        "    points_to_upsert.append(point)\n",
        "\n",
        "# 3. Upsert the points to Qdrant\n",
        "# SOLUTION (Part 4)\n",
        "# Upload the prepared points to your Qdrant collection.\n",
        "client.upsert(\n",
        "    collection_name=collection_name,\n",
        "    points=points_to_upsert,\n",
        "    wait=True\n",
        ")\n",
        "\n",
        "print(f\"Successfully embedded and upserted all {len(docs)} documents.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing local embedding models...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14273655b05d47b0a2b1f6eb6b73970a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9133f0ccdae74185b3e3e240faa99d5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6065d7df2bd47d7be0ec1b16b11fd0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a50d6589af041ecad1b60c82a5fb12f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b5b31fcd4a94170b1626565da99bdf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "259f473ac83e488eaf8b3beb065369e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95a03f72c2234975aa903160c7a5d37d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "484b517d4b804c3bb870ca3266c84c24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab9d30f9363f49f8aaa15f8a64b7610f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "442a24f9ee9b48adb6d79ca4a05549cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "907e308eb3814049a4003ec4b3e6d96c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc724fd94c704f08a371b7c11cd0d92c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd7f93f25d084935980c6bbe49ef2cdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02ae7056c74448b689ad5227fc8ec14e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.onnx:   0%|          | 0.00/532M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf68159711ee427984e806a90eef2d43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acf886b31d3841b8a049df87652f012a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "727370344bf34ec7ae6f64e4d6424010"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9aea0da3add41b5a0c303167c1c3de0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models initialized.\n",
            "Embedding and preparing all documents for upsert...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing All Documents:   0%|          | 0/191 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b8e918c95854b278200647fafb4f762"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded and upserted all 191 documents.\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693,
          "referenced_widgets": [
            "14273655b05d47b0a2b1f6eb6b73970a",
            "7920c8f9cf5c484599b832fd72ddafc9",
            "aba2d2b8df044b9e8601971e95f5bb11",
            "42b8d34a7f5944a7aeaabec8548ff6df",
            "698cc053a5694a67a037ab1f0897f151",
            "1332c64794414f0d80e3e22ab985aec6",
            "7a566ba8402e4e56bd14646bf3d53e08",
            "c7f0b0de0edf434c879c578eec4167d8",
            "5c9eb66e75cc450e93035d4d29915455",
            "801d5f0e8f284f7a99023dd99fa7a96d",
            "17584e023a064e53ab6fff1cfbe5f917",
            "9133f0ccdae74185b3e3e240faa99d5d",
            "d2e6b428277444d6acca4fa177c38f4b",
            "c71bcaec01e641e6ad0d1ffc137a7a67",
            "b1e5de8894ec44068db64a65c26de4f4",
            "c92bfd878df748b998d1f6155bfc4762",
            "42914aceb8704cc59d13969d8384f99a",
            "eda71d2c5dc648b784a0f79fec7b50b4",
            "79572d3672584f0399a9a800f63318d2",
            "4165208c5e66494fa7b91ec5bda5b82c",
            "d0335d21f8774469afd80e0ae71b960d",
            "f569b6662c8d488f8427dcb155165ff4",
            "c6065d7df2bd47d7be0ec1b16b11fd0c",
            "b64c388b6b384e7998dfdfc193cc8dfd",
            "90e278fa3bb14826804f8f605a748160",
            "3a4ee0bc3ae4486599d8e3ded4243d8c",
            "041932cca793485ab5b5759748cc1629",
            "1f3b297549b243408bf8c362a31ff52d",
            "0416b08735ef4ff38d7170c2906988d9",
            "5a136408eea24a768f687c5a96e673dc",
            "9f873635bb5840b686d7080c5be5c026",
            "329cbd60bb5e44e48ba8ef8a98369544",
            "3c02fffab4684791acdd5993feaf2640",
            "0a50d6589af041ecad1b60c82a5fb12f",
            "c6030e1fbaec4dee9eeca4ac7e8d3d63",
            "67a68aef6cfe465c86e6c960cc09b7a5",
            "b52a8b921ee94bbe852b23dc724e53e0",
            "381b6e2c8bdd44218e33f05ebe51e442",
            "de598cdf6f3846179140ce05f3820042",
            "8aa689776543403fa555ced337bf9a25",
            "9a636aaf11c34ba184b1455fed685a4b",
            "43214d277d974f2fa56e303657856acd",
            "a5b7eb6928b848daa234d29c35723258",
            "36eae24ce03342e18f788cd48a6daefb",
            "7b5b31fcd4a94170b1626565da99bdf3",
            "0076daa3e149497098610350b5f35dfc",
            "ee2d951732ac4bc2b4b67b2484f154b6",
            "1aa925218055493bbdb887ec89cec1cf",
            "2368263de9064e11a2732d614240a2fa",
            "5b85b274d3cd4846b850f1f997bb8d96",
            "0a52239549a04682b94b0ef537b4512a",
            "8b9a5144ef2c46978d818c6cecdd760d",
            "aa4d2c53ebc44f88b7b9e046dd26c250",
            "083263f2eacb45948560a5c620201ba2",
            "42c20d52879c481a8611e1bf528718ad",
            "259f473ac83e488eaf8b3beb065369e9",
            "306b06d7d81a47f3ae0e6b16ee3a95ac",
            "7c2a148670fa4424a955863cc6b76a50",
            "c38c7770bdf74f10b569ad259a408707",
            "0c82cacda4184c1082afbcdb4ec20d17",
            "bb01063a1e534ca59bbe4316d7fad474",
            "0634b34e9956443a8f602e2718f40188",
            "fc11725ffaf046aab1a3bc6d56c56ccc",
            "d31d7968355141ea8c1559eded2c16c5",
            "fa35ca19c8254e05870ed339ee1e0dc6",
            "88073c0b34274b01aa84466bc0b5e6eb",
            "95a03f72c2234975aa903160c7a5d37d",
            "d5a4ce58a033406d9397acb643f3f0b1",
            "21b2e3bf3aec4bf99be9c96ff40c5af8",
            "7a1a0e58e3314852a4e5d44ff65ddb85",
            "00b574b8703a4aaba82294787bbcee5e",
            "817011e369be4f10a1d611f044710d33",
            "0766c8a15f2243188f791db4e7fc6467",
            "b814926302394152b4e0962fb2ac1bc7",
            "5405176637e2480c8fad8b8433fe5f3a",
            "24dd23abcce04fd9aea8f95d3ad4f2f7",
            "60a4006e8aa346a4b43e50d9fce20c90",
            "484b517d4b804c3bb870ca3266c84c24",
            "a5c055c19e58404da35733526bd2c0f6",
            "7d07df3b160c4ecaa8a58f610aefd3e9",
            "503b825f137d40eeb0726a752a25693a",
            "0124d804cfac47b79b62f380757debdc",
            "c8d5f9d488044b16aef2491a1bc3484d",
            "962e850023b54475a7c93b40f839ce11",
            "23981f72ccea4929b728738b4da1b04a",
            "2eb1a76135af462398b0fb245d2a512d",
            "a630b4093771443992d45923a3954ffe",
            "0c2f6a86abe048ce87270c19bd82639b",
            "ab9d30f9363f49f8aaa15f8a64b7610f",
            "d2ecd2499eba4ee48278cfd072a741d4",
            "1e98c0ac77c74bce9637204081facd9e",
            "9d77da29e217443199ad9fae273e210d",
            "f7f50229ad884fa09d52a1c18d00cab6",
            "9e296b0d928548d492e0cf82955a595c",
            "16caa7f8076340b9b02f5483bc12aca6",
            "aa4d81a0730245dfa9bf87cbb11c73f8",
            "42560fbc387f454c90080ad46a88579b",
            "14e7ed722d514190b2a34291a1e779d6",
            "8dded0b7d97f43a1993702e39971ab27",
            "442a24f9ee9b48adb6d79ca4a05549cd",
            "b98aece9c89548dc80b03c24b3c9418c",
            "750d9f54f418428192c4562ce9f1eecb",
            "c605c8d2bf9844deb68d1445ab5f2eba",
            "1190a6a280764b6f9d54b1882fa30bef",
            "907796eee6c4468f9b884890dfb172b5",
            "7d80bcd3da9348c2af39a94ffa1357bd",
            "204a7dfdb08f4839ac332bbc646cc27c",
            "50b1d9bb42c447dc8419db11b4c2d8fa",
            "15ede60bce414a389bff7af988156b6d",
            "778cb51e8faf4dcdb89c19af542d9379",
            "907e308eb3814049a4003ec4b3e6d96c",
            "10af0a94685a4ba6a691117e7f773330",
            "458c3579c6cf4cdf85bb2221fc996c66",
            "5558cdbf10674cc19cba7c1bff9ca8fe",
            "84635e4aae7c423c8a45bbe8207e9852",
            "be61c195c751402d8f0f652b3dcfb109",
            "e6e679071cc549e7a25111312d1529df",
            "17c4a2f014af419081f5151ec4d5d03e",
            "abfbe14b0f5748159666f8edcd29e9f9",
            "51bfeed1d6dd4144b5fe3478db60e440",
            "b8bc6dc318a34fafa6c6eb8fc3258c5a",
            "bc724fd94c704f08a371b7c11cd0d92c",
            "f883979386614f0893d3f76627e828f5",
            "1d7044a578df45788113cb0006d0ce28",
            "c7b7d6adcd3a4f349e79dfe1760d313e",
            "de69b1301107410eba44e0945dc94f2d",
            "655f73bb0ec043e0aa98e4f483689ca7",
            "644e9b6b1bc84032b8ad16f4b035b447",
            "d9757316a503412f91005cf2037373c6",
            "ae5cb90f9e7743d88ea455bdc34c2790",
            "fb186ead0e0c4ef89252773f4133b1f7",
            "cc7e0588cd43472db3cb3caec7621328",
            "bd7f93f25d084935980c6bbe49ef2cdd",
            "b56035a778bc4e91b0a9b750b1afd51a",
            "3996a47a88414d2a93fc78f285519e32",
            "bbe86fac46de4af5b82ca74c9692c08c",
            "762705945ba44964af1814ad44063b59",
            "ff0f725a9b3b4057b56def1b877016a5",
            "9f29e663c25545c792f27891850a086d",
            "b84e41adcad6412f84ae0fdc45119397",
            "fb877fdfa8f04b90b6f80be4f5ad5c47",
            "06eed8a3a7714ddfba102e459f17ef34",
            "cba8507170f347848673aa2ca3b61fdc",
            "02ae7056c74448b689ad5227fc8ec14e",
            "1fde456679a341c98ab4e45aa91b2a62",
            "0aefab6ada6a4ce59c7a76fe0dd66584",
            "6eed9b88008940518719834bf6c7ee8b",
            "7c4200d2b39b4e90bd66d4217a0486d6",
            "52dbe97174ed42e2a3a080fa4c33eec1",
            "691f512532d149c8a21cb947e5258f35",
            "170f75e12b6e4c6c98903b5c812bc4a3",
            "6257b52c128e4c6c968bb7b573364bc3",
            "b92e1255a5a947bfba60ca6a1b52b360",
            "ca3941488d644be1b03313f06a5e18f3",
            "bf68159711ee427984e806a90eef2d43",
            "05f1e069b6654d309c1b411038f4f568",
            "8fec68e7f20847e48e2159707111732e",
            "2c90f084ca7d4f67a6052556f57e73af",
            "6fb36b8840594be585a29592f789d632",
            "4335e46ce445481c8e5ffdf926b49b02",
            "bc7305dd94ba49b6b85f4e0ecdc51e05",
            "8d1825a5aa2d4ea2b9f0ee3d93c488b5",
            "72906997fae4446fa955c1d92fb2a813",
            "4000a9fab5a042b6a55dfe48bcb02de2",
            "57854c5d8db84ba9b276c1e0882e58c7",
            "acf886b31d3841b8a049df87652f012a",
            "b0f293aa5e85428ca31906cd0f600e89",
            "f43dc9f79b404bdcaae21de040fc3a01",
            "702c447cdf8b4fc0890c7a3c6891584b",
            "47941dcf529842cdb257e3e62353f8be",
            "c891ebfec73144faa752513f7ad71c2c",
            "64f9aaad50024950a7aa37645405c3ed",
            "8f9122eb54cd4f659cbc26c0fd387202",
            "b738d39567db449898f1f178af9c8a50",
            "03966ccd5c5d4a1b8ab1a887099768cc",
            "92a3144eb8a24b1d9ebf0ac8e822c1b4",
            "727370344bf34ec7ae6f64e4d6424010",
            "dc59fde0a3a24e7593e3c124e0b80beb",
            "07191850c0844c0aac836b20a85717ac",
            "478948f5e54546efb8057a3b169bc09e",
            "8da8fc7b5e68484a89135b548e643b5f",
            "994dd2f8738e42009a0ca8a38622cbfe",
            "b3396be754484c349b9bf6a45a7ca639",
            "de87dbe59c654a288438574c689bcedb",
            "f1f6ccb09a204c938be43cd7f7739e45",
            "93bcb0cb18654de189abea5c9744eef3",
            "38a0d285c1dc4794aa22f8b608ad6d14",
            "c9aea0da3add41b5a0c303167c1c3de0",
            "dbad31fc4d7546609349b5a0c1203c06",
            "a85dadf5d48b440faa4f4d80abe6d063",
            "4889ea63e4b8478b83d00f8c272ed47b",
            "e2cb76cc315f45ea914a96f537a8f33c",
            "feecb725f8cd49558d76ddb227dc4cb1",
            "360e48dd3543471194ce654e7592757e",
            "2b25e02a6e4840acbd1efd46248b8131",
            "2023283922064e628e2e1c374dde9c09",
            "8cecd25832e144bf8d87862758c00572",
            "7b0b9612375f43cfaa7db263da898cba",
            "8b8e918c95854b278200647fafb4f762",
            "eb2127b7ed2843b4b7d2b941520600b5",
            "eb9d5720a5654acdb68b54b2b971c6e6",
            "19dfbbd184b44c469318e14056b006ad",
            "e8b752b826f84a52abd499ed66069310",
            "5881414f193d4ff988ae1fb9390cb77b",
            "9aec0c1b949941e58e664adeac9521f7",
            "579f3adf418d4623bf73361dc33aca3f",
            "c29dfc91df1341a49f103f45e0d92d66",
            "14cc0a73962a461aa68f9ec9e74d7b23",
            "d6976b724d1b4a269f252ef26127c1f3"
          ]
        },
        "id": "y6OuB_QyMfxb",
        "outputId": "9e88a4f3-e263-4bd9-f1ec-707a69964cf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 5: Build the Hybrid RAG Chain**\n",
        "\n",
        "Now we'll build our retrieval function. This function needs to perform two separate searches in Qdrant (one for dense vectors, one for sparse) and then intelligently combine the results before passing them to the LLM."
      ],
      "metadata": {
        "id": "nxnOscpLMfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize the Groq LLM\n",
        "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "# --- Helper function to visualize the context ---\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"Found {len(docs)} documents to pass to the LLM.\\n\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get('source', 'Unknown Source'); page = doc.metadata.get('page', 'Unknown Page')\n",
        "        print(f\"  [{i+1}] Source: {source} (Page: {page})\"); print(f\"      Content: '{doc.page_content[:150]}...'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- Custom Retrieval Function ---\n",
        "def qdrant_hybrid_retrieve(query: str, top_k=4) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Performs hybrid search and returns a list of LangChain Document objects.\n",
        "    We are deliberately changing k=2 or 4 to demonstrate recall failure.\n",
        "    \"\"\"\n",
        "    # SOLUTION (Part 1)\n",
        "    # Create the dense and sparse vectors for the input 'query'.\n",
        "\n",
        "    # This small tweak was added after switching embedding wrappers.\n",
        "    # Earlier, we were using `HuggingFaceBgeEmbeddings`, and the correct answer was retrieved even with k=4.\n",
        "    # After switching to the newer `HuggingFaceEmbeddings`, the same query failed to retrieve the answer.\n",
        "    # Adding the \"query: \" prefix resolved this issue and brought back the correct result at k=4.\n",
        "    query = f\"query: {query}\"\n",
        "\n",
        "\n",
        "    dense_query_vec = dense_embed_model.embed_query(query)\n",
        "    sparse_query_vec = list(sparse_embed_model.embed([query]))[0]\n",
        "\n",
        "    # SOLUTION (Part 2)\n",
        "    # Perform the two separate searches (dense and sparse) using the client.search() method.\n",
        "    dense_results = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=models.NamedVector(name=\"dense\", vector=dense_query_vec),\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "    sparse_results = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=models.NamedSparseVector(\n",
        "            name=\"text-sparse\",\n",
        "            vector=models.SparseVector(indices=sparse_query_vec.indices.tolist(), values=sparse_query_vec.values.tolist())\n",
        "        ),\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "\n",
        "    # Print dense retrieval results\n",
        "    print(f\"\\n--- Dense Search Results (k={top_k}) ---\")\n",
        "    dense_documents = []\n",
        "    for result in dense_results:\n",
        "        doc = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "        dense_documents.append(doc)\n",
        "    pretty_print_docs(dense_documents)\n",
        "\n",
        "    # Print sparse retrieval results\n",
        "    print(f\"\\n--- Sparse Search Results (k={top_k}) ---\")\n",
        "    sparse_documents = []\n",
        "    for result in sparse_results:\n",
        "        doc = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "        sparse_documents.append(doc)\n",
        "    pretty_print_docs(sparse_documents)\n",
        "\n",
        "    # --- RRF Fusion Logic ---\n",
        "    rrf_scores = {}\n",
        "    doc_lookup = {}\n",
        "    k_constant = 60  # The RRF constant 'k' dampens the influence of lower-ranked documents.\n",
        "\n",
        "\n",
        "    # --- Process Dense Search Results ---\n",
        "    # Iterate through each result from the dense (semantic) search, keeping track of its rank.\n",
        "\n",
        "    for rank, result in enumerate(dense_results):\n",
        "        # If this is the first time we've seen this document ID, initialize its score and store its content.\n",
        "        if result.id not in rrf_scores:\n",
        "            rrf_scores[result.id] = 0\n",
        "            doc_lookup[result.id] = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "        # Add the RRF score from the dense search results to the document's total score.\n",
        "        # The score is calculated as 1 / (k + rank).\n",
        "        rrf_scores[result.id] += 1 / (k_constant + rank + 1)\n",
        "\n",
        "    # --- Process Sparse Search Results ---\n",
        "    # Do the same for the sparse (keyword) search results.\n",
        "    for rank, result in enumerate(sparse_results):\n",
        "        # If we see a document for the first time, initialize it.\n",
        "        if result.id not in rrf_scores:\n",
        "            rrf_scores[result.id] = 0\n",
        "            doc_lookup[result.id] = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "\n",
        "        # Add the RRF score from the sparse search results to the document's total score.\n",
        "        # If a document appeared in both searches, its score will now be the sum of both calculations.\n",
        "        rrf_scores[result.id] += 1 / (k_constant + rank + 1)\n",
        "\n",
        "\n",
        "    # Sort documents by RRF score\n",
        "    sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
        "    combined_documents = [doc_lookup[doc_id] for doc_id in sorted_ids]\n",
        "\n",
        "    print(f\"\\n--- RRF Fusion Results (Hybrid Search with k={top_k}) ---\")\n",
        "    pretty_print_docs(combined_documents)\n",
        "\n",
        "    return combined_documents\n",
        "\n",
        "# --- Build the RAG Chain (This part is provided for you) ---\n",
        "prompt_template = \"Answer the question based only on the following context:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "rag_chain = (\n",
        "    {\"context\": qdrant_hybrid_retrieve, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
        ")\n",
        "print(\"RAG chain with Qdrant hybrid retrieval is ready.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG chain with Qdrant hybrid retrieval is ready.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mzskQJrMfxc",
        "outputId": "48f8e99c-1117-4b8c-bc1f-78a6d005d4b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 6: Test the Hybrid RAG Chain**\n",
        "\n",
        "This is the moment of truth. First, we will test the query that failed in Module 1 to see if our new hybrid search retriever has solved the problem. Then, we will try a new, more difficult query to see if we can find the limits of our current system."
      ],
      "metadata": {
        "id": "xrP_BKNqMfxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the Test Queries ---\n",
        "# This part is provided for you\n",
        "\n",
        "# Query #1: The query that failed in Module 1\n",
        "module_1_failure_query = \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
        "\n",
        "# Query #2: Our new, more difficult query for this module\n",
        "module_2_failure_query = \"What was the exact value for \\\"Tax withholding related to common stock from stock plans\\\" for the period ending April 27, 2025?\"\n",
        "\n",
        "print(\"--- Testing Query #1 (The Module 1 Failure) ---\")\n",
        "print(f\"Query: {module_1_failure_query}\\n\")\n",
        "answer_1 = rag_chain.invoke(module_1_failure_query)\n",
        "print('\\033[92m' + f\"Answer: {answer_1}\\n\" + '\\033[0m')\n",
        "print(\"-\" * 100)\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Testing Query #2 (Our New Challenge) ---\")\n",
        "print(f\"Query: {module_2_failure_query}\\n\")\n",
        "answer_2 = rag_chain.invoke(module_2_failure_query)\n",
        "print('\\033[91m' + f\"Answer: {answer_2}\\n\" + '\\033[0m')\n",
        "print(\"-\" * 100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing Query #1 (The Module 1 Failure) ---\n",
            "Query: How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\n",
            "\n",
            "\n",
            "--- Dense Search Results (k=4) ---\n",
            "Found 4 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 13)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Property and Equipment:\n",
            "Property, equi...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 16)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Total future purchase commitments as o...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 9)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Note 4 - Income Taxes\n",
            "Income tax expen...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 20)\n",
            "      Content: 'NVIDIA Corporation and SubsidiariesNotes to Condensed Consolidated Financial Statements (Continued)\n",
            "(Unaudited)\n",
            "Operating Lease Obligations\n",
            " (In milli...'\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sparse Search Results (k=4) ---\n",
            "Found 4 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 13)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Property and Equipment:\n",
            "Property, equi...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 16)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Total future purchase commitments as o...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 12)\n",
            "      Content: 'Future Amortization Expense\n",
            " (In millions)\n",
            "Fiscal Year:  \n",
            "2026 (excluding the first quarter of fiscal year 2026) $ 229 \n",
            "2027 276 \n",
            "2028 123 \n",
            "2029 38 \n",
            "2...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 35)\n",
            "      Content: 'We paid cash dividends to our shareholders of $244 million and $98 million during the first quarter of fiscal years 2026 and 2025, respectively. The p...'\n",
            "--------------------------------------------------\n",
            "\n",
            "--- RRF Fusion Results (Hybrid Search with k=4) ---\n",
            "Found 6 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 13)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Property and Equipment:\n",
            "Property, equi...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 16)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Total future purchase commitments as o...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 9)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Note 4 - Income Taxes\n",
            "Income tax expen...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 12)\n",
            "      Content: 'Future Amortization Expense\n",
            " (In millions)\n",
            "Fiscal Year:  \n",
            "2026 (excluding the first quarter of fiscal year 2026) $ 229 \n",
            "2027 276 \n",
            "2028 123 \n",
            "2029 38 \n",
            "2...'\n",
            "  [5] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 20)\n",
            "      Content: 'NVIDIA Corporation and SubsidiariesNotes to Condensed Consolidated Financial Statements (Continued)\n",
            "(Unaudited)\n",
            "Operating Lease Obligations\n",
            " (In milli...'\n",
            "  [6] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 35)\n",
            "      Content: 'We paid cash dividends to our shareholders of $244 million and $98 million during the first quarter of fiscal years 2026 and 2025, respectively. The p...'\n",
            "--------------------------------------------------\n",
            "\u001b[92mAnswer: To find out how much NVIDIA spent on share repurchases in the first quarter of fiscal year 2026, we need to calculate the total amount spent based on the information provided about the share repurchase transactions.\n",
            "\n",
            "The details of share repurchase transactions during the first quarter of fiscal year 2026 are as follows:\n",
            "\n",
            "1. January 27, 2025 - February 23, 2025: 28.6 million shares at $128.65 per share\n",
            "2. February 24, 2025 - March 23, 2025: 28.5 million shares at $118.63 per share\n",
            "3. March 24, 2025 - April 27, 2025: 69.1 million shares at $106.27 per share\n",
            "\n",
            "Let's calculate the total amount spent:\n",
            "\n",
            "1. 28.6 million shares * $128.65 = $3,679.39 million (approximately)\n",
            "2. 28.5 million shares * $118.63 = $3,382.955 million (approximately)\n",
            "3. 69.1 million shares * $106.27 = $7,341.897 million (approximately)\n",
            "\n",
            "Adding these amounts together: \n",
            "$3,679.39 + $3,382.955 + $7,341.897 = $14,404.242 million\n",
            "\n",
            "However, a more straightforward approach given the data provided is to use the \"Approximate Dollar Value of Shares that May Yet Be Purchased Under the Program\" and the \"Total Number of Shares Purchased\" to infer the expenditure. But directly, \n",
            "\n",
            "The total number of shares purchased is 126.2 million.\n",
            "\n",
            "The average price can be inferred from total and shares: \n",
            "- Total spent =  126.2 * Average price \n",
            "\n",
            "However, exact calculation directly from given data:\n",
            "January27,2025 - February23,2025: $128.65 * 28.6 = 3679.39\n",
            "February24,2025 - March23,2025: $118.63 * 28.5 = 3382.955 \n",
            "March24,2025 - April27,2025: $106.27* 69.1 = 7341.897\n",
            "\n",
            "Total = 3679.39 + 3382.955+ 7341.897= 14404.242 \n",
            "\n",
            "The closest and direct way to answer based on understanding  share value  $128.65, $118.63 and $106.27 average prices paid and shares bought 28.6M, 28.5M and 69.1M \n",
            "Thus Total spent  on 126.2  shares = 14404 Million or $ .  14.404Billion.\n",
            "\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "--- Testing Query #2 (Our New Challenge) ---\n",
            "Query: What was the exact value for \"Tax withholding related to common stock from stock plans\" for the period ending April 27, 2025?\n",
            "\n",
            "\n",
            "--- Dense Search Results (k=4) ---\n",
            "Found 4 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 5)\n",
            "      Content: 'Stock-based compensation — — 1,470 — — 1,470 \n",
            "Balances as of Apr 27, 2025 24,388 $ 24 $ 11,475 $ 186 $ 72,158 $ 83,843 \n",
            "Balances as of Jan 28, 2024 24...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 36)\n",
            "      Content: '(1)     Average price paid per share includes broker commissions, but excludes our liability under the 1% excise tax on the net amount of our share re...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 36)\n",
            "      Content: 'incentive program. During the first quarter of fiscal year 2026, we withheld approximately 13 million shares for a total value of $1.5 billion through...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 17)\n",
            "      Content: 'ultimate outcome of these actions will not have a material adverse effect on our operating results, liquidity or financial position.\n",
            "Note 12 - Shareho...'\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sparse Search Results (k=4) ---\n",
            "Found 4 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 5)\n",
            "      Content: 'Stock-based compensation — — 1,470 — — 1,470 \n",
            "Balances as of Apr 27, 2025 24,388 $ 24 $ 11,475 $ 186 $ 72,158 $ 83,843 \n",
            "Balances as of Jan 28, 2024 24...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 36)\n",
            "      Content: 'incentive program. During the first quarter of fiscal year 2026, we withheld approximately 13 million shares for a total value of $1.5 billion through...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 27)\n",
            "      Content: 'Capital Return to Shareholders\n",
            "We repurchased 126 million shares of our common stock for $14.5 billion during the first quarter of fiscal years 2026. ...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 17)\n",
            "      Content: 'ultimate outcome of these actions will not have a material adverse effect on our operating results, liquidity or financial position.\n",
            "Note 12 - Shareho...'\n",
            "--------------------------------------------------\n",
            "\n",
            "--- RRF Fusion Results (Hybrid Search with k=4) ---\n",
            "Found 5 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 5)\n",
            "      Content: 'Stock-based compensation — — 1,470 — — 1,470 \n",
            "Balances as of Apr 27, 2025 24,388 $ 24 $ 11,475 $ 186 $ 72,158 $ 83,843 \n",
            "Balances as of Jan 28, 2024 24...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 36)\n",
            "      Content: 'incentive program. During the first quarter of fiscal year 2026, we withheld approximately 13 million shares for a total value of $1.5 billion through...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 17)\n",
            "      Content: 'ultimate outcome of these actions will not have a material adverse effect on our operating results, liquidity or financial position.\n",
            "Note 12 - Shareho...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 36)\n",
            "      Content: '(1)     Average price paid per share includes broker commissions, but excludes our liability under the 1% excise tax on the net amount of our share re...'\n",
            "  [5] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 27)\n",
            "      Content: 'Capital Return to Shareholders\n",
            "We repurchased 126 million shares of our common stock for $14.5 billion during the first quarter of fiscal years 2026. ...'\n",
            "--------------------------------------------------\n",
            "\u001b[91mAnswer: The exact value for \"Tax withholding related to common stock from stock plans\" for the period ending April 27, 2025 was $(1,752).\n",
            "\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRMZXWGbMfxc",
        "outputId": "78107f5b-b960-4f1f-e0c4-4c985ca84afa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Conclusion: A Step Forward, and a Critical Failure\n",
        "\n",
        "After completing the notebook, you should see that the results from this module are a fantastic real-world lesson in building RAG systems.\n",
        "\n",
        "**1. A Major Success**: Our new Hybrid Search retriever has successfully solved the critical failure from Module 1. For the query about \"share repurchases,\" the system correctly found the relevant chunks and provided the right answer ($14.5 billion).\n",
        "\n",
        "This proves that by combining dense (semantic) and sparse (keyword) vectors, we can build a system with excellent recall—the ability to find relevant documents even when the query relies on specific keywords.\n",
        "\n",
        "**2. A New, More Subtle Failure**: However, you will see when we test it with our second, more difficult query, the system fails in a critical way.\n",
        "\n",
        "The Query: \\\"What was the exact value for 'Tax withholding related to common stock from stock plans' for the period ending April 27, 2025?\\\"\n",
        "\n",
        "The Result: The system returns the wrong value: $1,752 million (the value from the wrong year).\n",
        "\n",
        "**The Diagnosis**: A Recall Failure. This is not a case of the LLM getting confused. The root cause is that our retriever, with its narrow search of k=2 or 4, never finds the correct chunk of text from the document. The combination of a basic document loader (PyPDFLoader) that struggles with tables and a small k value means that the correct information from page 6 never passes to the LLM. The system retrieves other, less relevant chunks that happens to contain the wrong number.\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "**Hybrid Search** is a powerful tool, but it's not a magic bullet. The performance of a RAG pipeline is only as strong as its weakest link. We've just proven that even with a strong search algorithm, a poor chunking strategy combined with an overly narrow retrieval setting (k=2) can cause the entire system to fail. We have not yet built a truly high-recall system capable of handling this difficult query.\n",
        "\n",
        "### Next Up\n",
        "\n",
        "**In Module 3**, we'll implement a robust, **two-stage Retrieve and Re-rank** architecture to fix our system's precision issues. First, we'll solve the recall problem by using our **fast retriever** to cast a wider net (increasing k to 10), ensuring the correct documents are found, even if they're buried in noise.\n",
        "\n",
        "Then, we'll introduce a **Re-Ranker—an intelligent second stage** that analyzes these noisy results, promotes the single best answer to the top, and guarantees our LLM receives the cleanest possible context for generating an accurate response.\n",
        "\n",
        "For our learning path, we're tackling the re-ranker first to demonstrate a powerful technique for fixing an imprecise retriever, a common real-world challenge.\n",
        "\n",
        "However, it's critical to understand that the ideal production-grade solution is to use both a layout-aware parser and a re-ranker. The best practice is always to fix data quality at the source. Therefore, after mastering re-ranking, the perfect next step would be to replace our basic parser with a tool like **LlamaParse or Unstructured.io** to see how a clean data foundation can dramatically improve the entire system's efficiency and precision."
      ],
      "metadata": {
        "id": "axJG7ACJMfxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix A"
      ],
      "metadata": {
        "id": "QnqYaKgSCkyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Important Finding While Switching from Deprecated HuggingFaceBgeEmbeddings to the Newer HuggingFaceEmbeddings in LangChain ##\n",
        "\n",
        "🧪 Issue Summary: Query Retrieval Failure After Changing Embedding Wrapper\n",
        "\n",
        "Background\n",
        "  - Initially, I used HuggingFaceBgeEmbeddings with the model \"BAAI/bge-m3\" and was able to retrieve the correct document even with k=4 during hybrid search\n",
        "  - Later, I migrated to the newer recommended HuggingFaceEmbeddings wrapper from LangChain using the same model and parameters:\n",
        "\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "\n",
        "However, after this change, the same query failed to retrieve the correct document even at k=6.\n",
        "\n",
        "⸻\n",
        "\n",
        "🔎 Investigation\n",
        "  - HuggingFaceBgeEmbeddings is a wrapper that may be formatting queries internally — for example, it might be prepending the \"query: \" instruction prefix (especially if is_instruction=True, which is the default).\n",
        "  - On the other hand, HuggingFaceEmbeddings is a generic wrapper and does not apply any such formatting. It simply passes the query string as-is to the model.\n",
        "  - Since the BGE family of models (e.g., bge-m3) is instruction-tuned, they expect queries to be prefixed with \"query: \" in order to produce correct semantic embeddings.\n",
        "\n",
        "⸻\n",
        "\n",
        "✅ Fix\n",
        "\n",
        "To align with the expected format of instruction-tuned models, we updated the code to explicitly prepend \"query: \" before embedding:\n",
        "\n",
        "query = f\"query: {query}\"\n",
        "\n",
        "\n",
        "After applying this tweak, the correct document was retrieved again at k=4, restoring similar behavior to the original wrapper (though not guaranteed to be identical).\n",
        "\n",
        "⸻\n",
        "\n",
        "🧠 **What’s the Difference Between the Two Wrappers?**\n",
        "\n",
        "HuggingFaceBgeEmbeddings (Specialized Wrapper – Deprecated)\n",
        "\n",
        "  - Tailored specifically for the BGE family of models from the Beijing Academy of Artificial Intelligence.\n",
        "  - Likely includes model-specific behavior, such as auto-prepending instruction prefixes like \"query: \" to queries.\n",
        "  - Optimized for ease-of-use when working with instruction-tuned embedding models.\n",
        "\n",
        "HuggingFaceEmbeddings (General-Purpose Wrapper – Recommended)\n",
        "\n",
        "  - Designed to work universally with any SentenceTransformer-compatible model from the Hugging Face Hub.\n",
        "  - Does not make assumptions about the model’s formatting requirements.\n",
        "  - Does not add instruction prefixes, making it more flexible but requiring the developer to handle formatting when needed (e.g., for instruction-tuned models like BGE)."
      ],
      "metadata": {
        "id": "jnLLhrUX1dU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Reciprocal Rank Fusion (RRF)\n",
        "\n",
        "### The \"Two Movie Critics\" Analogy for RRF\n",
        "\n",
        "Imagine you have two expert movie critics who you trust:\n",
        "* **Critic A (Our Dense Search):** This critic is great at understanding the *feeling* and *themes* of a movie.\n",
        "* **Critic B (Our Sparse Search):** This critic is excellent at catching specific details and keywords in the dialogue.\n",
        "\n",
        "You ask them both to recommend the top 3 movies about \"space exploration.\"\n",
        "\n",
        "#### Step 1: Get the Two Lists\n",
        "\n",
        "The critics come back with slightly different ranked lists:\n",
        "\n",
        "**Critic A (Dense/Semantic) Results:**\n",
        "1.  *Galaxy Quest* (A great parody, captures the *feeling* of exploration)\n",
        "2.  *Apollo 13* (About a real mission)\n",
        "3.  *The Martian* (Focuses on survival)\n",
        "\n",
        "**Critic B (Keyword) Results:**\n",
        "1.  *Apollo 13* (Uses the exact keyword \"space exploration\")\n",
        "2.  *Interstellar* (About exploring new galaxies)\n",
        "3.  *The Martian* (Also about space)\n",
        "\n",
        "#### Step 2: The RRF Code in Action\n",
        "\n",
        "Now, let's see what our RRF code does with these two lists.\n",
        "\n",
        "1.  **It creates an empty scoreboard (`rrf_scores`) and a library of the movies (`doc_lookup`).**\n",
        "\n",
        "2.  **It processes Critic A's list:**\n",
        "    * *Galaxy Quest* is ranked #1, so it gets a high score (e.g., `1 / (60 + 1)`).\n",
        "    * *Apollo 13* is ranked #2, so it gets a slightly lower score (e.g., `1 / (60 + 2)`).\n",
        "    * *The Martian* is ranked #3, so it gets an even lower score (e.g., `1 / (60 + 3)`).\n",
        "\n",
        "3.  **It processes Critic B's list:**\n",
        "    * *Apollo 13* is ranked #1. It's already on our scoreboard, so we **add** more points to its score. It now has a very high total score!\n",
        "    * *Interstellar* is ranked #2. It's new, so it gets its first score (e.g., `1 / (60 + 2)`).\n",
        "    * *The Martian* is ranked #3. It's already on our scoreboard, so we **add** more points to its existing score.\n",
        "\n",
        "#### Step 3: The Final Fused Ranking\n",
        "\n",
        "After adding up all the points, our final scoreboard looks something like this (higher score is better):\n",
        "\n",
        "1.  **Apollo 13:** (High score from Critic A + Highest score from Critic B) -> **Highest Score**\n",
        "2.  **The Martian:** (Medium score from Critic A + Medium score from Critic B) -> **High Score**\n",
        "3.  **Galaxy Quest:** (Highest score from Critic A + No score from Critic B) -> **Good Score**\n",
        "4.  **Interstellar:** (No score from Critic A + High score from Critic B) -> **Good Score**\n",
        "\n",
        "The code then sorts the movies by this new RRF score. The final, fused list it sends to the LLM would be: `[Apollo 13, The Martian, Galaxy Quest, Interstellar]`.\n",
        "\n",
        "This shows how RRF intelligently promotes the documents that **both** search methods agree are important, giving us a much more reliable and relevant final ranking."
      ],
      "metadata": {
        "id": "sVPv30j_Cnay"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
