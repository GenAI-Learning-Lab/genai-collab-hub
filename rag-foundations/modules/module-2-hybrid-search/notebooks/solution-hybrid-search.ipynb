{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Solution Notebook: Module 2 - Hybrid Search**\n",
        "\n",
        "*This notebook contains the solutions for the guided hands-on exercise.*\n",
        "\n",
        "-----\n",
        "\n",
        "### **Module 2: Improving Recall with Hybrid Search**\n",
        "\n",
        "**Objective:**\n",
        "In our first module, we saw a critical **Recall Failure**. Our basic RAG system, using only semantic search, completely missed the correct document chunk for a query about \"share repurchases.\" It failed to find the right information in the knowledge base.\n",
        "\n",
        "The objective of this module is to solve that recall problem by implementing a more powerful **Hybrid Search** system. We will combine traditional keyword-based search with the semantic search we've already learned. This will create a much more reliable retriever.\n",
        "\n",
        "**Learning Objectives:**\n",
        "By the end of this module, you will be able to:\n",
        "- Explain the core concept of Hybrid Search and understand the distinct roles of dense (semantic) and sparse (keyword) vectors.\n",
        "- Implement a hybrid data strategy by creating both dense and sparse embeddings for your documents using open-source models.\n",
        "- Configure and populate a Qdrant collection that handles a sophisticated hybrid search workload.\n",
        "- Build a custom retrieval function that performs both dense and sparse searches and fuses the results.\n",
        "- Diagnose a **Recall Failure** and understand why a narrow search (`k=4`) can cause the system to fail, even with a better algorithm.\n",
        "\n",
        "**Core Concept: Hybrid Search with Qdrant**\n",
        "We will create and store two types of vectors for each document chunk:\n",
        "1.  **Dense Vector (from `bge-m3`):** Captures the *semantic meaning* and conceptual relationships.\n",
        "2.  **Sparse Vector (from `Splade`):** Captures the *keyword importance*.\n",
        "\n",
        "When a query comes in, our system will perform two separate searches—one for meaning and one for keywords—and then combine the results. This gives us the best of both worlds, making our system far more robust against the type of keyword-based failure we saw in Module 1.\n"
      ],
      "metadata": {
        "id": "QaVFG2yHMfxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Install Dependencies**"
      ],
      "metadata": {
        "id": "CyQKsdC9Q06f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries\n",
        "!pip install -q langchain langchain-community langchain-groq qdrant-client pypdf fastembed\n",
        "\n",
        "# Ignore standard warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGTx86A0MfxY",
        "outputId": "9c6c3fe5-7ca1-4640-d98d-3e9cc59bd5a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 2: Setup API Key & Document Loading**\n",
        "\n",
        "\n",
        "\n",
        "This step remains the same as Module 1. In this module, we reuse our Module-1 API keys, we load the NVIDIA financial report PDF, and split it into chunks."
      ],
      "metadata": {
        "id": "AC_txeIoMfxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Setup API Key ---\n",
        "# Make sure you have added your GROQ_API_KEY to the Colab secrets manager\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# --- Load and Split Document ---\n",
        "# Make sure you have uploaded the NVIDIA Q1 FY26 PDF to your Colab session\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Use the same chunking strategy as Module 1\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document loaded and split into 191 chunks.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Q9cJMMMfxa",
        "outputId": "1a3864c1-cf9b-480b-ba76-a3c839ade22e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 3: Initialize Qdrant for Hybrid Search**\n",
        "\n",
        "This is a key step. We will create a Qdrant client and then create a new **collection** that is specifically configured to handle both dense and sparse vectors. This is different from Module 1 where we only had one type of vector.\n"
      ],
      "metadata": {
        "id": "dZMs6FiCMfxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "# Initialize an in-memory Qdrant client\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"rag_foundations_m2_guided\"\n",
        "\n",
        "# Create the collection with configurations for both dense and sparse vectors\n",
        "print(f\"Creating Qdrant collection '{collection_name}' for hybrid search...\")\n",
        "\n",
        "# SOLUTION\n",
        "client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config={\n",
        "        \"dense\": models.VectorParams(size=1024, distance=models.Distance.COSINE)\n",
        "    },\n",
        "    sparse_vectors_config={\n",
        "        \"text-sparse\": models.SparseVectorParams(\n",
        "            index=models.SparseIndexParams(\n",
        "                on_disk=False\n",
        "            )\n",
        "        )\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Collection created successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Qdrant collection 'rag_foundations_m2_guided' for hybrid search...\n",
            "Collection created successfully.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0cfxsiUMfxa",
        "outputId": "abb73eaa-1c30-42fd-f5c2-a40a5946ca9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 4: Embed and Store Documents**\n",
        "\n",
        "\n",
        "Now we will perform the main data processing. We will loop through every document chunk, create both a dense and a sparse vector for it, and then store them together in our new Qdrant collection."
      ],
      "metadata": {
        "id": "whFRJtskMfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from fastembed import SparseTextEmbedding\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Initializing local embedding models...\")\n",
        "# 1. Initialize our embedding models\n",
        "dense_embed_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "sparse_embed_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
        "print(\"Models initialized.\")\n",
        "\n",
        "# 2. Embed and prepare all documents for upsert\n",
        "print(\"Embedding and preparing all documents for upsert...\")\n",
        "points_to_upsert = []\n",
        "for i, doc in enumerate(tqdm(docs, desc=\"Processing All Documents\")):\n",
        "    doc_text = doc.page_content\n",
        "\n",
        "    # SOLUTION (Part 1)\n",
        "    # Create the dense vector for the doc_text.\n",
        "    dense_vec = dense_embed_model.embed_query(doc_text)\n",
        "\n",
        "    # SOLUTION (Part 2)\n",
        "    # Create the sparse vector for the doc_text.\n",
        "    sparse_vec = list(sparse_embed_model.embed([doc_text]))[0]\n",
        "\n",
        "    # SOLUTION (Part 3)\n",
        "    # Create a Qdrant PointStruct to hold all the data.\n",
        "    point = models.PointStruct(\n",
        "        id=i,\n",
        "        payload={\"text\": doc_text, **doc.metadata},\n",
        "        vector={\n",
        "            \"dense\": dense_vec,\n",
        "            \"text-sparse\": models.SparseVector(\n",
        "                indices=sparse_vec.indices.tolist(),\n",
        "                values=sparse_vec.values.tolist()\n",
        "            ),\n",
        "        },\n",
        "    )\n",
        "\n",
        "    points_to_upsert.append(point)\n",
        "\n",
        "# 3. Upsert the points to Qdrant\n",
        "# SOLUTION (Part 4)\n",
        "# Upload the prepared points to your Qdrant collection.\n",
        "client.upsert(\n",
        "    collection_name=collection_name,\n",
        "    points=points_to_upsert,\n",
        "    wait=True\n",
        ")\n",
        "\n",
        "print(f\"Successfully embedded and upserted all {len(docs)} documents.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing local embedding models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1594638512.py:7: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  dense_embed_model = HuggingFaceBgeEmbeddings(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74d0c020ceb64e7cbdd08b314a32ec68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17f4236def2a46f8ac76df39cbe02a61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "694b8e23f4df45dd888bc7b285e0daad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cadd30039154987bc4e378b32ebff00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "948aa0f3f9c648edbe9eb217aaec0e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6497793c953407ca5ae71250421e704"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcc5d62d81b94757b43813e916057f8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f947d8b208324d6491e6bff10bea53a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd0ea4198c74666a988c7d4e45d8260"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6483cad4ebdf46daba690a1086e33491"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "375e599baaf540fb8630ced7d6036693"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a98cdddf23a048e8b4d2e743047663ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c7c08d528fd470c91f68c94acc0c344"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a7c2642972949f795d3993cc84b362c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9cae144d7b74315abe549a11893665c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e43e8159f8247bf87eed5e594eb4e85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbbeaa159d004b2db67212feaf973770"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.onnx:   0%|          | 0.00/532M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88691194c28944a38ab8e7fc990855b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models initialized.\n",
            "Embedding and preparing all documents for upsert...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing All Documents:   0%|          | 0/191 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7064f6aefbf41d8a1381df42e393cee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded and upserted all 191 documents.\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747,
          "referenced_widgets": [
            "74d0c020ceb64e7cbdd08b314a32ec68",
            "c6221297a0fc4249a891c5d0aa222794",
            "6b579b25afec4bd7b1e8132ae8e847e1",
            "785d7b33946c4b54a7404a58b9448e36",
            "d69f0d923c3941e0a8f686e287ce43ad",
            "7df4c2952f3c49fd98bfceea26d9c406",
            "ea6ec2562e4342e4b4c6b7e19b2e9979",
            "6326a9c3da6d4b5482671804cae08d4a",
            "b441fca685a3472080d02d83a60801c6",
            "8dbd06df6ff941619fcbd67fd1a0bf6c",
            "cf620af2b7004d628fff13d6a3cf6b1c",
            "17f4236def2a46f8ac76df39cbe02a61",
            "ca5ce809653e420198b1f11b8a748b56",
            "76a6f505e3f949a993197bc9ab7ec484",
            "56049df55cd84e0f994300124a3c6e8e",
            "16785f1560a34e5bb4bcc408bc94ca21",
            "61b956a8ee1c4295b089d272e7b63bb8",
            "af8f62976ed141cd9f2fd08cf655fe48",
            "7317be42f60141d7a126e60da8bc8e67",
            "74bb48ab429f4fdcb1cedcbaeb5ed621",
            "6941459362404b07929e060e9b140c02",
            "33ca0ae08591496ca02425aa890eb8f3",
            "694b8e23f4df45dd888bc7b285e0daad",
            "22435166f9644287a87cfa1ac2099827",
            "0e206055994442279e73f7289c813dca",
            "7da13d38ec314a87aeb282dac36d6d9d",
            "c42e6d6da33442f0aae8dbd660ef4729",
            "f37084200fd0430cb396172c15a67a61",
            "1a47f3a7d141451b992e391589a8e1ce",
            "17a598b0722a4de8913ea3fbd1808b69",
            "afd67da4df80477898bcc1a7cd1e90fe",
            "6e5727a471ec4a2886dc18da043bda0d",
            "cdca4ef660cd4e5c9ed5f5b7fc0dbf97",
            "4cadd30039154987bc4e378b32ebff00",
            "857657429f1b41f7b7bdb4b6811433de",
            "6561bde5156e4ff6beda2ccc28c81205",
            "5a748d1f1e4f4ac1946406384b6f56ce",
            "342e62803076401499ff94e3f5d21672",
            "50ddc935d0f8474aad8d4523f2db2de7",
            "74e29d364da3449b97e34945290aa9ea",
            "3333f76a3dfa4eae99b6309a3c90c4aa",
            "6f3b4dafee404968a2d9e6c53a188373",
            "3321303f35e8403797b3d279be4a4b58",
            "17f12917ea784a858b1b864034a2063c",
            "948aa0f3f9c648edbe9eb217aaec0e3e",
            "554da0c59ba04a0bae6a63c0cdc704f6",
            "853a076e2a0f4d68b16b2da55587f7bb",
            "0d44af7f42f54ec1a839e35ed15b6281",
            "aeff89e5bb3f47eaa7b9af9c2712ac18",
            "cfab2070c6e54951b210ed18b87f2d10",
            "cfb90323abd54a1487d9bee53ee63e61",
            "59e2f46dc7d244c08ba21ed356bde7a7",
            "bb01559eea7c4916bce9b422ba640096",
            "509e939ed1ed4e19935f5da52532057b",
            "4e5f7ff823fa4569af83e88ff06fd545",
            "e6497793c953407ca5ae71250421e704",
            "bf5ae7710aa544e7acf2ffc96969fa6b",
            "1ef68bea0ce6456f831638bdf5e178fc",
            "8ca04274872d47b8ac1d2d07d49b3a4a",
            "da60c426629a4335aabda16ee5ef0c98",
            "ff186e9165ad4da4a7b62d7cc3d2c107",
            "43045b807b7e4bdcbd80ccaca3dda7ca",
            "f5bc14cc8c9049dc847fa25511f1cd56",
            "179d57ff86b2484181605ff34418a2f0",
            "39110a08158c4e8795547ad4d2a4d5b0",
            "4406b58a39dd45a2b3cfe79fe43aeb3d",
            "fcc5d62d81b94757b43813e916057f8d",
            "7d623d4125124034bc3c8dd98fae876f",
            "17485a2d1d56454b81d75d0874355218",
            "3f2727b1caea48b682bd2afb156c8671",
            "c267f57eb2114dc5b9297a1476b9519c",
            "dcb1182d90bf4467ad92baef73046d0a",
            "a22791408b4d4068be014fb61a60f003",
            "19ba0b39bec54c68b6c2b9feceb0f9b1",
            "36529aa5861c4e04b10974a29bd7324b",
            "4396b413cd7a4c00b618ce076abe14ca",
            "52243dcfbf59407ea74d844ddeece2c2",
            "f947d8b208324d6491e6bff10bea53a0",
            "d46540954b77429d9f7a48dabaf94257",
            "8d23e76d602f4bd2bb299324b9b20051",
            "cdd09e5f15314c55a12a46051b2c61d9",
            "6a91e3995ab040e4afb54cf566c08009",
            "ba76066ce3c74c95b19b19627d4d74f5",
            "1e39d15c166644b2a0c9146539449b37",
            "1f5ed8eba877400cac9bbc63516d3ece",
            "d051e89ceace4664b2ff8aecf0eaff3c",
            "9595c07978314978a940d5fc1fc61ace",
            "68d77bee0c894142b21ff84cb2337220",
            "3dd0ea4198c74666a988c7d4e45d8260",
            "8f000131fd804a1a8c2e9976cdd3d4cf",
            "872b8e1b274f420eb9dac276e84b3109",
            "d7b3f479a3134637a59e8c64867f0ee6",
            "541744fcdfa54b6c8d2f712d75e79346",
            "ed74a3b255bf4240ac5ae304f93e4f98",
            "7deb0fe706334e298c8553d5705a5375",
            "f3a5006956f14129a0591fdded19cbb4",
            "216db63744d740bf92ea3bf563eabdba",
            "6be790db63bd46dab6174e7b36c8824b",
            "ccc6895365ce478fb892a780495055cf",
            "6483cad4ebdf46daba690a1086e33491",
            "5b374a20227c41eca7ae4105043424c4",
            "f65a63d56edb4b5fbf153ed9c474e7c1",
            "68299335fe284065933c73a7acfb3ee1",
            "103501de38de47c6ad5e97c49c244d28",
            "33ea13faaa8d42f4be8341cb7df1b313",
            "24858ccb995148a18c576a4bd64b0801",
            "61f3220e3aad43d4bc85f0ec694db83b",
            "8b9368f25b09446eb2232fdff52f758f",
            "1c4b29810e354feb8c727891388831a2",
            "7b5c138e0b494e4a94bcd44bb998ec9f",
            "375e599baaf540fb8630ced7d6036693",
            "cc44f8f4adbf4daba99ea1bd4283d509",
            "97c1c5ac184e40febc1be5a88643876b",
            "95bcad748bf34949b02bd2b209d03959",
            "5086baf8d3e6463c984d94f0ef856097",
            "c472b3bae4a241aa9e0cb750f2bae462",
            "f7a84e8f4817466b9bbdc9b7f7aa6b4b",
            "1180b447095547c3a9f30bb42a781eac",
            "857c8a88dd784b7d9869caf953014d6d",
            "b771f2828a4e4abda3b0ba677664ee1c",
            "022a67ef37a04349aaf1ba6184160da3",
            "a98cdddf23a048e8b4d2e743047663ef",
            "97e301a4ddcd4f899cc74f1cf1f4ce14",
            "c84e9429457b427fb33bf6f501d31fb9",
            "d48a2560b7754abb92d96ceac418161f",
            "478fe21a459f425694ac88f3ce0871ca",
            "f27d221be665435d9d9828d95f9bbc78",
            "5b2ae6355b3047fc867d1727f16f5e08",
            "bc6a65a3f9f545669fe4db111d089463",
            "ed921d4bdce54b91bd51379567c391dd",
            "64abb5cdf657441fb8d41cdb766af594",
            "e597f0d0c26d407289f4033338610280",
            "9c7c08d528fd470c91f68c94acc0c344",
            "a74bc9bd959b47328bc0bb1c12b7b8e5",
            "f7c8355e3e294f54a3277f3807970eb1",
            "191977e24f8b4171a3edaed453827837",
            "da710daf11fb41b888fa8ad62221ff67",
            "a50347713b264d20ba685a972ff5b8da",
            "031b53833a574204a928f47d18e0f108",
            "6f2e7cbde4c741d1a6a620ce98f54431",
            "919bb3cae8a84ebab6462f3c51ea29c2",
            "a6335276d55847b8b35d2c2504ed07c4",
            "f5da4a09cb0d4394bcbcb0c86d688ce3",
            "8a7c2642972949f795d3993cc84b362c",
            "3d3671e7369f4cd691783c61ed4d3a1b",
            "651fd731462d4a20be516dfbd533b247",
            "113a2eac23414dcda1ec93ad983e6b46",
            "c6499ffc10d1461aaa2ad597e89d9aa7",
            "113f459ba5334dd38eaaecdeee396b91",
            "4e2df1b804444f208a37121a4e44d1ce",
            "f8be22eace0c4b108c6141c4fcd83bd5",
            "b23ff8057b6c41d88f31a6bc81d04d52",
            "e3295688d7674936a89d329a065f6244",
            "e2f52df7c1224c7fa78399dd3e310d59",
            "d9cae144d7b74315abe549a11893665c",
            "30578b35f32b4c548974e8dc441f74bb",
            "b1fe60dcda6b4bad8f98278f4d3c440d",
            "95997c5b12184b3b94b2432cfd29290a",
            "42e5fb1ac3d44bd391c000e937e60ee6",
            "85a39d127cde49caa6b3a9143923cad5",
            "e77e17bb7cdc41ef8fdb76c505a1f41c",
            "14c5d0e9d2214872a351cbd95a2358c2",
            "5d31fa88b365470292740ad22358de33",
            "31989657c9b14b4785860fe60a3367d8",
            "b8b0237cba5045c484912ba617db14e1",
            "2e43e8159f8247bf87eed5e594eb4e85",
            "faf24b33de4d4bd5bcbdb8cd0cb0272a",
            "c1e63831450f4b5dbf0c1977f573660e",
            "cee6645de3304737b2383b916d6e4eb4",
            "705f2990fc4745e78b000c21fb6407b8",
            "f082b03bbfc34f8fb4756c7a24deb7ce",
            "3125afb91e3c48ffbceba1eb0a41ad32",
            "cb6b178276cc44f485e9c131c6e55ef3",
            "e4b8e853503c444d922aa00aea1307d1",
            "99c7facfc47b48beac7135a9bf3dab83",
            "f44978dcde6940a094d31bb07546338d",
            "dbbeaa159d004b2db67212feaf973770",
            "f27e25314d554d7ba9cce5cdd360e116",
            "a451e78f45464f2791fec9201a8c0755",
            "7d7d283662734d7a8e6f6fd3120a3f5a",
            "5317971d9e834b8d81fd76272437717d",
            "4e3a9e754b584352b7774485b99b0eb0",
            "018a335be6b04d2c9aed01e9fde1633c",
            "a984833c2328424b830d17cafcb649e8",
            "522f43186fef4bea843e57b03d8bbeeb",
            "3911b5083b9b4998a55e1d081e139559",
            "4cc4019ef783499a9a65d411a55e962e",
            "88691194c28944a38ab8e7fc990855b3",
            "e232511eadf14d088869065a6fba2d13",
            "c608e0512ede457f8a32f80336d34367",
            "cb48b427f67e4ea4b58817a8e18bb4fd",
            "3b36f52c468348d390cdd4d7e9f6e051",
            "96087f77d6244bf3a586f31e19cc5cc4",
            "682cf4c99666404f872542885bfd0835",
            "2de3315ffdc440f588d81e59098a6622",
            "68fae87d37fa405faa1c342b3984d922",
            "0dcb7fb7d5cf42cb9c761e3bc5e77ff3",
            "9de83ef4b0ba4d57bcc29310f87f404b",
            "b7064f6aefbf41d8a1381df42e393cee",
            "ccacfce290de443fbdc32c66d9bb6a54",
            "fe960a4907524e79b70cb97d771f9b9e",
            "fb86b2a0c137416b9d85e4991fdbf564",
            "fe3797d00ef8480381fe053cddba9d8e",
            "cdf1431f5584472db7cd699298e00243",
            "321d0de5800b445e8a96cc79d89024d9",
            "d6b5728df8014ff1b19a5adc320c4473",
            "7a9a43d60d304852b7ae2d0a405fd1c6",
            "4d597c24b7f240468ea9f641c5b219b5",
            "e65ced9f997c413a90f285904c099b1a"
          ]
        },
        "id": "y6OuB_QyMfxb",
        "outputId": "ebfce675-ee75-4008-e06a-400efe36623a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 5: Build the Hybrid RAG Chain**\n",
        "\n",
        "Now we'll build our retrieval function. This function needs to perform two separate searches in Qdrant (one for dense vectors, one for sparse) and then intelligently combine the results before passing them to the LLM."
      ],
      "metadata": {
        "id": "nxnOscpLMfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize the Groq LLM\n",
        "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "# --- Helper function to visualize the context ---\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"Found {len(docs)} documents to pass to the LLM.\\n\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get('source', 'Unknown Source'); page = doc.metadata.get('page', 'Unknown Page')\n",
        "        print(f\"  [{i+1}] Source: {source} (Page: {page})\"); print(f\"      Content: '{doc.page_content[:150]}...'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- Custom Retrieval Function ---\n",
        "def qdrant_hybrid_retrieve(query: str, top_k=2) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Performs hybrid search and returns a list of LangChain Document objects.\n",
        "    We are deliberately changing k=2 or 4 to demonstrate recall failure.\n",
        "    \"\"\"\n",
        "    # SOLUTION (Part 1)\n",
        "    # Create the dense and sparse vectors for the input 'query'.\n",
        "    dense_query_vec = dense_embed_model.embed_query(query)\n",
        "    sparse_query_vec = list(sparse_embed_model.embed([query]))[0]\n",
        "\n",
        "    # SOLUTION (Part 2)\n",
        "    # Perform the two separate searches (dense and sparse) using the client.search() method.\n",
        "    dense_results = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=models.NamedVector(name=\"dense\", vector=dense_query_vec),\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "    sparse_results = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=models.NamedSparseVector(\n",
        "            name=\"text-sparse\",\n",
        "            vector=models.SparseVector(indices=sparse_query_vec.indices.tolist(), values=sparse_query_vec.values.tolist())\n",
        "        ),\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "\n",
        "    # The fusion logic is provided for you\n",
        "    seen_ids = set()\n",
        "    combined_documents = []\n",
        "    all_results = dense_results + sparse_results\n",
        "    for result in all_results:\n",
        "        if result.id not in seen_ids:\n",
        "            doc = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "            combined_documents.append(doc)\n",
        "            seen_ids.add(result.id)\n",
        "\n",
        "    print(f\"--- Context Being Passed to LLM (from Hybrid Search with k={top_k}) ---\")\n",
        "    pretty_print_docs(combined_documents)\n",
        "\n",
        "    return combined_documents\n",
        "\n",
        "# --- Build the RAG Chain (This part is provided for you) ---\n",
        "prompt_template = \"Answer the question based only on the following context:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "rag_chain = (\n",
        "    {\"context\": qdrant_hybrid_retrieve, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
        ")\n",
        "print(\"RAG chain with Qdrant hybrid retrieval is ready.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG chain with Qdrant hybrid retrieval is ready.\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mzskQJrMfxc",
        "outputId": "58f01d10-00ae-470b-ef86-2285bf9fa9aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 6: Test the Hybrid RAG Chain**\n",
        "\n",
        "This is the moment of truth. First, we will test the query that failed in Module 1 to see if our new hybrid search retriever has solved the problem. Then, we will try a new, more difficult query to see if we can find the limits of our current system."
      ],
      "metadata": {
        "id": "xrP_BKNqMfxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the Test Queries ---\n",
        "# This part is provided for you\n",
        "\n",
        "# Query #1: The query that failed in Module 1\n",
        "module_1_failure_query = \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
        "\n",
        "# Query #2: Our new, more difficult query for this module\n",
        "module_2_failure_query = \"What was the exact value for \\\"Tax withholding related to common stock from stock plans\\\" for the period ending April 27, 2025?\"\n",
        "\n",
        "print(\"--- Testing Query #1 (The Module 1 Failure) ---\")\n",
        "print(f\"Query: {module_1_failure_query}\\n\")\n",
        "answer_1 = rag_chain.invoke(module_1_failure_query)\n",
        "print('\\033[92m' + f\"Answer: {answer_1}\\n\" + '\\033[0m')\n",
        "print(\"-\" * 100)\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Testing Query #2 (Our New Challenge) ---\")\n",
        "print(f\"Query: {module_2_failure_query}\\n\")\n",
        "answer_2 = rag_chain.invoke(module_2_failure_query)\n",
        "print('\\033[91m' + f\"Answer: {answer_2}\\n\" + '\\033[0m')\n",
        "print(\"-\" * 100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing Query #1 (The Module 1 Failure) ---\n",
            "Query: How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\n",
            "\n",
            "--- Context Being Passed to LLM (from Hybrid Search with k=2) ---\n",
            "Found 4 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 6)\n",
            "      Content: 'NVIDIA Corporation and Subsidiaries\n",
            "Condensed Consolidated Statements of Cash Flows(In millions)(Unaudited)\n",
            " Three Months Ended\n",
            " Apr 27, 2025 Apr 28, ...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 5)\n",
            "      Content: 'NVIDIA Corporation and SubsidiariesCondensed Consolidated Statements of Shareholders' Equity\n",
            "(Unaudited)\n",
            "Common StockOutstanding AdditionalPaid-in Cap...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 13)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Property and Equipment:\n",
            "Property, equi...'\n",
            "  [4] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 16)\n",
            "      Content: 'NVIDIA CORPORATION AND SUBSIDIARIESNOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
            "(Unaudited)\n",
            "Total future purchase commitments as o...'\n",
            "--------------------------------------------------\n",
            "\u001b[92mAnswer: According to the second document, in the first quarter of fiscal year 2026, NVIDIA repurchased 126 million shares. The total amount spent on share repurchases was $14,503 million.\n",
            "\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "--- Testing Query #2 (Our New Challenge) ---\n",
            "Query: What was the exact value for \"Tax withholding related to common stock from stock plans\" for the period ending April 27, 2025?\n",
            "\n",
            "--- Context Being Passed to LLM (from Hybrid Search with k=2) ---\n",
            "Found 3 documents to pass to the LLM.\n",
            "\n",
            "  [1] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 5)\n",
            "      Content: 'Stock-based compensation — — 1,470 — — 1,470 \n",
            "Balances as of Apr 27, 2025 24,388 $ 24 $ 11,475 $ 186 $ 72,158 $ 83,843 \n",
            "Balances as of Jan 28, 2024 24...'\n",
            "  [2] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 27)\n",
            "      Content: 'Capital Return to Shareholders\n",
            "We repurchased 126 million shares of our common stock for $14.5 billion during the first quarter of fiscal years 2026. ...'\n",
            "  [3] Source: ./NVIDIA-Q1-FY26-Financial-Results.pdf (Page: 36)\n",
            "      Content: 'incentive program. During the first quarter of fiscal year 2026, we withheld approximately 13 million shares for a total value of $1.5 billion through...'\n",
            "--------------------------------------------------\n",
            "\u001b[91mAnswer: The exact value for \"Tax withholding related to common stock from stock plans\" for the period ending April 27, 2025 was $(1,752).\n",
            "\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRMZXWGbMfxc",
        "outputId": "a5c7131c-cc9f-4a8f-c4b9-d2d2d0e1194e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Conclusion: A Step Forward, and a Critical Failure\n",
        "\n",
        "After completing the notebook, you should see that the results from this module are a fantastic **real-world lesson in building RAG systems.**\n",
        "\n",
        "**1. A Major Success:**\n",
        "Our new Hybrid Search retriever has **successfully solved the critical failure from Module 1**. For the query about \"share repurchases,\" the system correctly found the relevant chunks and provided the right answer ($14.5 billion).\n",
        "\n",
        "This proves that by combining dense (semantic) and sparse (keyword) vectors, we can build a system with excellent **recall**—the ability to find relevant documents even when the query relies on specific keywords.\n",
        "\n",
        "**2. A New, More Subtle Failure:**\n",
        "However, you will see when we test it with our second, more difficult query, the system fails in a critical way.\n",
        "\n",
        "* **The Query:** `\\\"What was the exact value for 'Tax withholding related to common stock from stock plans' for the period ending April 27, 2025?\\\"`\n",
        "* **The Result:** The system returns the wrong value: **$1,752 million** (the value from the wrong year).\n",
        "* **The Diagnosis: A Recall Failure.** This is not a case of the LLM getting confused. The root cause is that our retriever, with its narrow search of `k=2 or 4`, **never finds the correct chunk of text from the document.** The combination of a basic document loader (`PyPDFLoader`) that struggles with tables and a small `k` value means that the correct information from page 6 never passes to the LLM. The system retrieves other, less relevant chunks that happens to contain the wrong number.\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "Hybrid Search is a powerful tool, but it's not a magic bullet. The performance of a RAG pipeline is only as strong as its weakest link. We've just proven that even with a strong search algorithm, a poor **chunking strategy** combined with an overly **narrow retrieval setting (`k=2`)** can cause the entire system to fail. We have not yet built a truly high-recall system capable of handling this difficult query.\n",
        "\n",
        "### Next Up\n",
        "\n",
        "In **Module 3**, we'll implement a robust, two-stage Retrieve and Re-rank architecture to fix our system's precision issues. First, we'll solve the recall problem by using our fast retriever to cast a wider net (increasing k to 10), ensuring the correct documents are found, even if they're buried in noise. Then, we'll introduce a **Re-Ranker**—an intelligent second stage that analyzes these noisy results, promotes the single best answer to the top, and guarantees our LLM receives the cleanest possible context for generating an accurate response.\n",
        "\n",
        "For our learning path, we're tackling the re-ranker first to demonstrate a powerful technique for fixing an imprecise retriever, a common real-world challenge. However, it's critical to understand that the ideal production-grade solution is to use **both a layout-aware parser and a re-ranker**. The best practice is always to fix data quality at the source. Therefore, after mastering re-ranking, the perfect next step would be to replace our basic parser with a tool like LlamaParse or Unstructured.io to see how a clean data foundation can dramatically improve the entire system's efficiency and precision."
      ],
      "metadata": {
        "id": "axJG7ACJMfxc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
