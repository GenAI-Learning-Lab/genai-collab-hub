{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Guided Notebook: Module 2 - Hybrid Search**\n",
        "\n",
        "*This notebook contains the guided hands-on exercise. Fill in the `... # YOUR CODE HERE` sections to complete the module.*\n",
        "\n",
        "-----\n",
        "\n",
        "### **Module 2: Improving Recall with Hybrid Search**\n",
        "\n",
        "**Objective:**\n",
        "In our first module, we saw a critical **Recall Failure**. Our basic RAG system, using only semantic search, completely missed the correct document chunk for a query about \"share repurchases.\" It failed to find the right information in the knowledge base.\n",
        "\n",
        "The objective of this module is to solve that recall problem by implementing a more powerful **Hybrid Search** system. We will combine traditional keyword-based search with the semantic search we've already learned. This will create a much more reliable retriever.\n",
        "\n",
        "**Learning Objectives:**\n",
        "By the end of this module, you will be able to:\n",
        "- Explain the core concept of Hybrid Search and understand the distinct roles of dense (semantic) and sparse (keyword) vectors.\n",
        "- Implement a hybrid data strategy by creating both dense and sparse embeddings for your documents using open-source models.\n",
        "- Configure and populate a Qdrant collection that handles a sophisticated hybrid search workload.\n",
        "- Build a custom retrieval function that performs both dense and sparse searches and fuses the results using **Reciprocal Rank Fusion (RRF)**.\n",
        "- Diagnose a **Recall Failure** and understand why a narrow search (`k=4`) can cause the system to fail, even with a better algorithm.\n",
        "\n",
        "**Core Concept: Hybrid Search with Qdrant**\n",
        "We will create and store two types of vectors for each document chunk:\n",
        "1.  **Dense Vector (from `bge-m3`):** Captures the *semantic meaning* and conceptual relationships.\n",
        "2.  **Sparse Vector (from `Splade`):** Captures the *keyword importance*.\n",
        "\n",
        "When a query comes in, our system will perform two separate searches—one for meaning and one for keywords—and then combine the results. This gives us the best of both worlds, making our system far more robust against the type of keyword-based failure we saw in Module 1.\n"
      ],
      "metadata": {
        "id": "QaVFG2yHMfxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Install Dependencies**"
      ],
      "metadata": {
        "id": "CyQKsdC9Q06f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries\n",
        "!pip install -q langchain langchain-community langchain-groq langchain_huggingface qdrant-client pypdf fastembed\n",
        "\n",
        "# Ignore standard warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "DGTx86A0MfxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 2: Setup API Key & Document Loading**\n",
        "\n",
        "This step remains the same as Module 1. We set up our API key, load the NVIDIA financial report PDF, and split it into chunks."
      ],
      "metadata": {
        "id": "AC_txeIoMfxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Setup API Key ---\n",
        "# Make sure you have added your GROQ_API_KEY to the Colab secrets manager\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# --- Load and Split Document ---\n",
        "# Make sure you have uploaded the NVIDIA Q1 FY26 PDF to your Colab session\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Use the same chunking strategy as Module 1\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "35Q9cJMMMfxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 3: Initialize Qdrant for Hybrid Search**\n",
        "\n",
        "This is a key step. We will create a Qdrant client and then create a new **collection** that is specifically configured to handle both dense and sparse vectors. This is different from Module 1 where we only had one type of vector.\n"
      ],
      "metadata": {
        "id": "dZMs6FiCMfxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "# Initialize an in-memory Qdrant client\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"rag_foundations_m2_guided\"\n",
        "\n",
        "# --- Best Practice: Check if collection exists before creating ---\n",
        "if client.collection_exists(collection_name=collection_name):\n",
        "    print(f\"Collection '{collection_name}' already exists. Deleting it to start fresh.\")\n",
        "    client.delete_collection(collection_name=collection_name)\n",
        "\n",
        "print(f\"Creating Qdrant collection '{collection_name}' for hybrid search...\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Use the client.create_collection() method.\n",
        "# You need to configure two types of vectors inside the collection:\n",
        "# 1. A 'dense' vector using models.VectorParams with a size of 1024 and 'DOT' distance.\n",
        "# 2. A 'text-sparse' sparse vector using models.SparseVectorParams.\n",
        "# HINT: The structure should look like: client.create_collection(collection_name=..., vectors_config=..., sparse_vectors_config=...)\n",
        "...\n",
        "\n",
        "print(\"Collection created successfully.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "u0cfxsiUMfxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 4: Embed and Store Documents**\n",
        "\n",
        "Now we will perform the main data processing. We will loop through every document chunk, create both a dense and a sparse vector for it, and then store them together in our new Qdrant collection."
      ],
      "metadata": {
        "id": "whFRJtskMfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from fastembed import SparseTextEmbedding\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Initializing local embedding models...\")\n",
        "# 1. Initialize our embedding models\n",
        "dense_embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "sparse_embed_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
        "print(\"Models initialized.\")\n",
        "\n",
        "# 2. Embed and prepare all documents for upsert\n",
        "print(\"Embedding and preparing all documents for upsert...\")\n",
        "points_to_upsert = []\n",
        "for i, doc in enumerate(tqdm(docs, desc=\"Processing All Documents\")):\n",
        "    doc_text = doc.page_content\n",
        "\n",
        "    # YOUR CODE HERE (Part 1)\n",
        "    # Create the dense vector for 'doc_text' using the 'dense_embed_model'.\n",
        "    # HINT: Use the .embed_query() method.\n",
        "    dense_vec = ...\n",
        "\n",
        "    # YOUR CODE HERE (Part 2)\n",
        "    # Create the sparse vector for 'doc_text' using the 'sparse_embed_model'.\n",
        "    # HINT: The .embed() method returns a generator, so you must convert it to a list first.\n",
        "    sparse_vec = ...\n",
        "\n",
        "    # YOUR CODE HERE (Part 3)\n",
        "    # Create a Qdrant PointStruct to hold all the data.\n",
        "    # It needs an id, a payload (with the text and metadata), and a vector dictionary.\n",
        "    # The vector dictionary should have keys 'dense' and 'text-sparse'.\n",
        "    # For the sparse vector, you must convert its indices and values to a list.\n",
        "    # HINT: models.PointStruct(id=..., payload=..., vector={'dense':..., 'text-sparse':...})\n",
        "    point = ...\n",
        "\n",
        "    points_to_upsert.append(point)\n",
        "\n",
        "# 3. Upsert the points to Qdrant\n",
        "# YOUR CODE HERE (Part 4)\n",
        "# Upload the prepared points to your Qdrant collection.\n",
        "# HINT: Use the client.upsert() method.\n",
        "...\n",
        "\n",
        "print(f\"Successfully embedded and upserted all {len(docs)} documents.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "y6OuB_QyMfxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 5: Build the Hybrid RAG Chain with RRF**\n",
        "\n",
        "Now we'll build our retrieval function. This function performs two separate searches in Qdrant (one for dense vectors, one for sparse) and then intelligently combines the results using **Reciprocal Rank Fusion (RRF)** before passing them to the LLM."
      ],
      "metadata": {
        "id": "nxnOscpLMfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize the Groq LLM\n",
        "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "# --- Helper function to visualize the context ---\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"Found {len(docs)} documents to pass to the LLM.\\n\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get('source', 'Unknown Source'); page = doc.metadata.get('page', 'Unknown Page')\n",
        "        print(f\"  [{i+1}] Source: {source} (Page: {page})\"); print(f\"      Content: '{doc.page_content[:150]}...'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- Custom Retrieval Function with RRF ---\n",
        "def qdrant_hybrid_retrieve_rrf(query: str, top_k=4) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Performs hybrid search and returns a list of LangChain Document objects\n",
        "    fused with Reciprocal Rank Fusion (RRF).\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE (Part 1)\n",
        "    # Create the dense and sparse vectors for the input 'query'.\n",
        "    # Remember to handle the instruction prefix for the dense model!\n",
        "    # HINT: query = f\"query: {query}\"\n",
        "    ...\n",
        "    dense_query_vec = ...\n",
        "    sparse_query_vec = ...\n",
        "\n",
        "    # YOUR CODE HERE (Part 2)\n",
        "    # Perform the two separate searches (dense and sparse) using the client.search() method.\n",
        "    # Remember to use models.NamedVector and models.NamedSparseVector to specify which vector to search.\n",
        "    dense_results = ...\n",
        "    sparse_results = ...\n",
        "\n",
        "    # --- RRF Fusion Logic (This part is provided for you) ---\n",
        "    rrf_scores = {}\n",
        "    doc_lookup = {}\n",
        "    k_constant = 60\n",
        "\n",
        "    # Process dense results\n",
        "    for rank, result in enumerate(dense_results):\n",
        "        if result.id not in rrf_scores:\n",
        "            rrf_scores[result.id] = 0\n",
        "            doc_lookup[result.id] = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "        rrf_scores[result.id] += 1 / (k_constant + rank + 1)\n",
        "\n",
        "    # Process sparse results\n",
        "    for rank, result in enumerate(sparse_results):\n",
        "        if result.id not in rrf_scores:\n",
        "            rrf_scores[result.id] = 0\n",
        "            doc_lookup[result.id] = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "        rrf_scores[result.id] += 1 / (k_constant + rank + 1)\n",
        "\n",
        "    sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
        "    combined_documents = [doc_lookup[doc_id] for doc_id in sorted_ids]\n",
        "\n",
        "    print(f\"\\n--- RRF Fusion Results (Hybrid Search with k={top_k}) ---\")\n",
        "    pretty_print_docs(combined_documents)\n",
        "\n",
        "    return combined_documents\n",
        "\n",
        "# --- Build the RAG Chain (This part is provided for you) ---\n",
        "def format_docs(docs):\n",
        "    return \"\\n---\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt_template = \"Answer the question based only on the following context:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "rag_chain = (\n",
        "    {\"context\": qdrant_hybrid_retrieve_rrf, \"question\": RunnablePassthrough()} | \n",
        "    {\"context\": (lambda x: format_docs(x['context'])), \"question\": (lambda x: x['question'])} | \n",
        "    prompt | \n",
        "    llm | \n",
        "    StrOutputParser()\n",
        ")\n",
        "print(\"RAG chain with Qdrant hybrid retrieval is ready.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8mzskQJrMfxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 6: Test the Hybrid RAG Chain**\n",
        "\n",
        "This is the moment of truth. First, we will test the query that failed in Module 1 to see if our new hybrid search retriever has solved the problem. Then, we will try our new, more difficult query to see if we can find the limits of our current system."
      ],
      "metadata": {
        "id": "xrP_BKNqMfxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the Test Queries ---\n",
        "# This part is provided for you\n",
        "\n",
        "# Query #1: The query that failed in Module 1\n",
        "module_1_failure_query = \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
        "\n",
        "# Query #2: Our new, more difficult query for this module\n",
        "module_2_failure_query = \"What was the exact value for \\\"Tax withholding related to common stock from stock plans\\\" for the period ending April 27, 2025?\"\n",
        "\n",
        "print(\"--- Testing Query #1 (The Module 1 Failure) ---\")\n",
        "print(f\"Query: {module_1_failure_query}\\n\")\n",
        "answer_1 = rag_chain.invoke(module_1_failure_query)\n",
        "print('\\033[92m' + f\"Answer: {answer_1}\\n\" + '\\033[0m')\n",
        "print(\"-\" * 100)\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Testing Query #2 (Our New Challenge) ---\")\n",
        "print(f\"Query: {module_2_failure_query}\\n\")\n",
        "answer_2 = rag_chain.invoke(module_2_failure_query)\n",
        "print('\\033[91m' + f\"Answer: {answer_2}\\n\" + '\\033[0m')\n",
        "print(\"-\" * 100)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zRMZXWGbMfxc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
