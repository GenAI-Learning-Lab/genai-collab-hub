{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Guided Notebook: Module 2 - Hybrid Search**\n",
        "\n",
        "**Improving Recall with Hybrid Search**\n",
        "\n",
        "*This notebook is designed for learners to complete. Code sections marked with `YOUR CODE HERE` are to be filled in by the learner.*\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "**Objective:**\n",
        "In our first module, we saw a critical **Recall Failure**. Our basic RAG system, using only semantic search, completely missed the correct document chunk for a query about \"share repurchases.\" It failed to find the right information in the knowledge base.\n",
        "\n",
        "The objective of this module is to solve that recall problem by implementing a more powerful **Hybrid Search** system. We will combine traditional keyword-based search with the semantic search we've already learned. This will create a much more reliable retriever.\n",
        "\n",
        "**Learning Objectives:**\n",
        "By the end of this module, you will be able to:\n",
        "- Explain the core concept of Hybrid Search and understand the distinct roles of dense (semantic) and sparse (keyword) vectors.\n",
        "- Implement a hybrid data strategy by creating both dense and sparse embeddings for your documents using open-source models.\n",
        "- Configure and populate a Qdrant collection that handles a sophisticated hybrid search workload.\n",
        "- Build a custom retrieval function that performs both dense and sparse searches and fuses the results.\n",
        "- Diagnose a **Recall Failure** and understand why a narrow search (`k=4`) can cause the system to fail, even with a better algorithm.\n",
        "\n",
        "**Core Concept: Hybrid Search with Qdrant**\n",
        "We will create and store two types of vectors for each document chunk:\n",
        "1.  **Dense Vector (from `bge-m3`):** Captures the *semantic meaning* and conceptual relationships.\n",
        "2.  **Sparse Vector (from `Splade`):** Captures the *keyword importance*.\n",
        "\n",
        "When a query comes in, our system will perform two separate searches—one for meaning and one for keywords—and then combine the results. This gives us the best of both worlds, making our system far more robust against the type of keyword-based failure we saw in Module 1."
      ],
      "metadata": {
        "id": "mZvdV1UA_hbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Step 1: Install Dependencies**"
      ],
      "metadata": {
        "id": "8-kxnYpGDrkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries\n",
        "!pip install -q langchain langchain-community langchain-groq qdrant-client pypdf fastembed\n",
        "\n",
        "# Ignore standard warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "pkvG2MLO_hb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 2: Setup API Key & Document Loading**\n",
        "\n",
        "This step remains the same as Module 1. In this module, we reuse our Module-1 API keys, we load the NVIDIA financial report PDF, and split it into chunks."
      ],
      "metadata": {
        "id": "7Mj7LYFQ_hb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Setup API Key ---\n",
        "# Make sure you have added your GROQ_API_KEY to the Colab secrets manager\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# --- Load and Split Document ---\n",
        "# Make sure you have uploaded the NVIDIA Q1 FY26 PDF to your Colab session\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Use the same chunking strategy as Module 1\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "WMi06m5B_hb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 3: Initialize Qdrant for Hybrid Search**\n",
        "\n",
        "\n",
        "This is a key step. We will create a Qdrant client and then create a new **collection** that is specifically configured to handle both dense and sparse vectors. This is different from Module 1 where we only had one type of vector."
      ],
      "metadata": {
        "id": "gYNExnsv_hb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "# Initialize an in-memory Qdrant client\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"rag_foundations_m2_guided\"\n",
        "\n",
        "# Create the collection with configurations for both dense and sparse vectors\n",
        "print(f\"Creating Qdrant collection '{collection_name}' for hybrid search...\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Task: Configure and create the Qdrant collection.\n",
        "# HINT: Use the client.recreate_collection() method.\n",
        "# You need to configure two types of vectors inside the 'vectors_config' and 'sparse_vectors_config' arguments.\n",
        "#   1. A 'dense' vector using models.VectorParams. Set the size to 1024 (for bge-m3) and the distance to models.Distance.DOT.\n",
        "#   2. A 'text-sparse' sparse vector using models.SparseVectorParams.\n",
        "# The final structure should look like: client.recreate_collection(collection_name=..., vectors_config={...}, sparse_vectors_config={...})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Collection created successfully.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "DJiTvqTL_hb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 4: Embed and Store Documents**\n",
        "\n",
        "Now we will perform the main data processing. We will loop through every document chunk, create both a dense and a sparse vector for it, and then store them together in our new Qdrant collection.\n"
      ],
      "metadata": {
        "id": "FfuDBRsF_hb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from fastembed import SparseTextEmbedding\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Initializing local embedding models...\")\n",
        "# 1. Initialize our embedding models\n",
        "dense_embed_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "sparse_embed_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
        "print(\"Models initialized.\")\n",
        "\n",
        "# 2. Embed and prepare all documents for upsert\n",
        "print(\"Embedding and preparing all documents for upsert...\")\n",
        "points_to_upsert = []\n",
        "for i, doc in enumerate(tqdm(docs, desc=\"Processing All Documents\")):\n",
        "    doc_text = doc.page_content\n",
        "\n",
        "    # YOUR CODE HERE (Part 1)\n",
        "    # Task: Create the dense vector for the doc_text.\n",
        "    # HINT: Use the dense_embed_model.embed_query() method.\n",
        "    dense_vec = # ... complete this line\n",
        "\n",
        "    # YOUR CODE HERE (Part 2)\n",
        "    # Task: Create the sparse vector for the doc_text.\n",
        "    # HINT: The sparse_embed_model.embed() method returns a list, so you'll need the first element.\n",
        "    sparse_vec = # ... complete this line\n",
        "\n",
        "    # YOUR CODE HERE (Part 3)\n",
        "    # Task: Create a Qdrant PointStruct to hold all the data.\n",
        "    # HINT: It needs an 'id', a 'payload' (with the text and metadata), and a 'vector' dictionary.\n",
        "    # The vector dictionary should have keys \"dense\" and \"text-sparse\".\n",
        "    # For the sparse vector, you will need to convert its indices and values to a list.\n",
        "    # e.g., models.SparseVector(indices=sparse_vec.indices.tolist(), values=sparse_vec.values.tolist())\n",
        "    point = # ... complete this line\n",
        "\n",
        "    points_to_upsert.append(point)\n",
        "\n",
        "# 3. Upsert the points to Qdrant\n",
        "# YOUR CODE HERE (Part 4)\n",
        "# Task: Upload the prepared points to your Qdrant collection.\n",
        "# HINT: Use the client.upsert() method, passing the collection_name and the points_to_upsert list.\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Successfully embedded and upserted all {len(docs)} documents.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AjLQsQLt_hb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 5: Build the Hybrid RAG Chain**\n",
        "\n",
        "\n",
        "Now we'll build our retrieval function. This function needs to perform two separate searches in Qdrant (one for dense vectors, one for sparse) and then intelligently combine the results before passing them to the LLM.\n"
      ],
      "metadata": {
        "id": "Rtx14Q5N_hb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize the Groq LLM\n",
        "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "# --- Helper function to visualize the context ---\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"Found {len(docs)} documents to pass to the LLM.\\n\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get('source', 'Unknown Source'); page = doc.metadata.get('page', 'Unknown Page')\n",
        "        print(f\"  [{i+1}] Source: {source} (Page: {page})\"); print(f\"      Content: '{doc.page_content[:150]}...'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- Custom Retrieval Function ---\n",
        "def qdrant_hybrid_retrieve(query: str, top_k=4) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Performs hybrid search and returns a list of LangChain Document objects.\n",
        "    We are deliberately keeping k=4 to demonstrate recall failure.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE (Part 1)\n",
        "    # Task: Create the dense and sparse vectors for the input 'query'.\n",
        "    # This is the same process as in the previous step.\n",
        "    dense_query_vec = # ... complete this line\n",
        "    sparse_query_vec = # ... complete this line\n",
        "\n",
        "    # YOUR CODE HERE (Part 2)\n",
        "    # Task: Perform the two separate searches (dense and sparse) using the client.search() method.\n",
        "    # Remember to set the limit=top_k and with_payload=True.\n",
        "    # For each search, you must specify which vector you are querying against ('dense' or 'text-sparse').\n",
        "    dense_results = # ... complete this line\n",
        "    sparse_results = # ... complete this line\n",
        "\n",
        "    # The fusion logic is provided for you\n",
        "    seen_ids = set()\n",
        "    combined_documents = []\n",
        "    all_results = dense_results + sparse_results\n",
        "    for result in all_results:\n",
        "        if result.id not in seen_ids:\n",
        "            doc = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "            combined_documents.append(doc)\n",
        "            seen_ids.add(result.id)\n",
        "\n",
        "    print(\"--- Context Being Passed to LLM (from Hybrid Search with k=4) ---\")\n",
        "    pretty_print_docs(combined_documents)\n",
        "\n",
        "    return combined_documents\n",
        "\n",
        "# --- Build the RAG Chain (This part is provided for you) ---\n",
        "\n",
        "prompt_template = \"Answer the question based only on the following context:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": qdrant_hybrid_retrieve, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain with Qdrant hybrid retrieval is ready.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "d_ZomPsG_hb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Test the Hybrid RAG Chain**"
      ],
      "metadata": {
        "id": "TalT5KhZDb5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the Test Queries ---\n",
        "# This part is provided for you\n",
        "\n",
        "# Query #1: The query that failed in Module 1\n",
        "module_1_failure_query = \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
        "\n",
        "# Query #2: Our new, more difficult query for this module\n",
        "module_2_failure_query = \"What was the exact value for Tax withholding related to common stock from stock plans for the period ending April 27, 2025?\"\n",
        "\n",
        "print(\"--- Testing Query #1 (The Module 1 Failure) ---\")\n",
        "print(f\"Query: {module_1_failure_query}\\\\n\")\n",
        "answer_1 = rag_chain.invoke(module_1_failure_query)\n",
        "print(f\"Answer: {answer_1}\\\\n\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "\n",
        "print(\"\\\\n\\\\n--- Testing Query #2 (Our New Challenge) ---\")\n",
        "print(f\"Query: {module_2_failure_query}\\\\n\")\n",
        "answer_2 = rag_chain.invoke(module_2_failure_query)\n",
        "print(f\"Answer: {answer_2}\\\\n\")\n",
        "print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "B1YzDVHvDYV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 Conclusion: A Step Forward, and a Critical Failure\n",
        "\n",
        "After completing the notebook, you should see that the results from this module are a fantastic **real-world lesson in building RAG systems.**\n",
        "\n",
        "**1. A Major Success:**\n",
        "Our new Hybrid Search retriever will **successfully solve the critical failure from Module 1**. For the query about \"share repurchases,\" the system will correctly found the relevant chunks and provided the right answer ($14.5 billion).\n",
        "\n",
        "This proves that by combining dense (semantic) and sparse (keyword) vectors, we can build a system with excellent **recall**—the ability to find relevant documents even when the query relies on specific keywords.\n",
        "\n",
        "**2. A New, More Subtle Failure:**\n",
        "However, you will see when we test it with our second, more difficult query, the system fails in a critical way.\n",
        "\n",
        "* **The Query:** `\"What was the exact value for 'Tax withholding related to common stock from stock plans' for the period ending April 27, 2025?\"`\n",
        "* **The Result:** The system returns the wrong value: **$1,752 million** (the value from the wrong year).\n",
        "* **The Diagnosis: A Recall Failure.** This is not a case of the LLM getting confused. The root cause is that our retriever, with its narrow search of `k=4`, **never finds the correct chunk of text from the document.** The combination of a basic document loader (`PyPDFLoader`) that struggles with tables and a small `k` value means that the correct information from page 6 never passes to the LLM. The system retrieves other, less relevant chunks that happens to contain the wrong number.\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "Hybrid Search is a powerful tool, but it's not a magic bullet. The performance of a RAG pipeline is only as strong as its weakest link. We've just proven that even with a strong search algorithm, a poor **chunking strategy** combined with an overly **narrow retrieval setting (`k=4`)** can cause the entire system to fail. We have not yet built a truly high-recall system capable of handling this difficult query.\n",
        "\n",
        "### Next Up\n",
        "\n",
        "In **Module 3**, we will solve this problem by implementing a more robust, two-stage architecture. First, we will solve the **recall** problem by casting a wider net (increasing `k` to 10). Then, we will solve the resulting **precision** problem by implementing a **Re-Ranker**—an intelligent filter designed to analyze the noisy results and promote the single best answer to the top, ensuring our LLM always gets the cleanest, most direct context."
      ],
      "metadata": {
        "id": "KxZKGW3S_hb1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}