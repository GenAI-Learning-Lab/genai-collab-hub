{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Guided Notebook: Module 3 - Improving Precision with a Re-Ranker**\n",
        "\n",
        "*This notebook is designed for learners to complete. Code sections marked with `YOUR CODE HERE` are to be filled in by the learner.*\n",
        "\n",
        "-----\n",
        "\n",
        "### **An Architect's Note: The \"Why\" Behind Our `k` Values**\n",
        "\n",
        "Before we dive in, let's address a critical design choice for this module. Astute learners will notice that our Module 2 pipeline (with `k=4`) failed to find the correct document, while the pipeline we're building in this module uses an initial retrieval of `k=10`.\n",
        "\n",
        "You might ask: \"Aren't we just fixing the problem by using a larger `k`?\"\n",
        "\n",
        "The answer is: **Yes, but that's only the first half of the solution.** This is a deliberate choice to demonstrate the two distinct problems every advanced RAG system must solve:\n",
        "\n",
        "1.  **The Recall Problem:** In Module 2, our system suffered from a Recall Failure. The search was too narrow (`k=4`) and couldn't find the correct document chunk in the first place. The first step to building a robust system is to solve this by casting a wider net (`k=10`), ensuring the correct information is almost certainly in our initial candidate pool.\n",
        "\n",
        "2.  **The Precision Problem:** Casting a wider net solves the recall issue, but it creates a new problem: our candidate list is now much larger and noisier. If we were to pass this entire messy list to the LLM, we would be creating a Precision Failure, where the LLM gets confused and likely provides the wrong answer.\n",
        "\n",
        "This module is designed to solve the second, more subtle problem. We will first solve for recall by increasing `k`, and then we will implement a **Re-Ranker** as an \"intelligent filter\" to provide the high precision needed for a trustworthy and reliable answer. This two-stage \"wide net, then precise filter\" approach is the core architectural pattern you will learn here.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Step 1: Install Dependencies**\n",
        "\n",
        "The dependencies are the same as our previous module."
      ],
      "metadata": {
        "id": "hZ84wSKdVFb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community langchain-groq qdrant-client sentence-transformers pypdf fastembed"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "7053xML6VFb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-dziPn04VFcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 2: Setup (API Key, Document Loading, and Qdrant Population)**\n",
        "\n",
        "This cell contains all the setup code from Module 2. It will load the document, create the dense and sparse embeddings, and populate our in-memory Qdrant collection. We will process the **full document** this time to ensure our system is robust."
      ],
      "metadata": {
        "id": "_oQs6OKpVFcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from fastembed import SparseTextEmbedding\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 1. Setup API Key ---\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# --- 2. Load and Split Document ---\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")\n",
        "\n",
        "# --- 3. Initialize Qdrant Client and Collection ---\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "collection_name = \"rag_foundations_qdrant_hybrid\"\n",
        "client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config={\n",
        "        \"dense\": models.VectorParams(size=1024, distance=models.Distance.DOT)\n",
        "    },\n",
        "    sparse_vectors_config={\n",
        "        \"text-sparse\": models.SparseVectorParams(index=models.SparseIndexParams(on_disk=False))\n",
        "    }\n",
        ")\n",
        "print(\"Qdrant collection created.\")\n",
        "\n",
        "# --- 4. Initialize Embedding Models ---\n",
        "dense_embed_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "sparse_embed_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
        "print(\"Embedding models initialized.\")\n",
        "\n",
        "# --- 5. Embed and Upsert Full Document ---\n",
        "points_to_upsert = []\n",
        "for i, doc in enumerate(tqdm(docs, desc=\"Processing and Upserting All Docs\")):\n",
        "    doc_text = doc.page_content\n",
        "    dense_vec = dense_embed_model.embed_query(doc_text)\n",
        "    sparse_vec = list(sparse_embed_model.embed([doc_text]))[0]\n",
        "    points_to_upsert.append(\n",
        "        models.PointStruct(\n",
        "            id=i,\n",
        "            payload={\"text\": doc_text, **doc.metadata},\n",
        "            vector={\n",
        "                \"dense\": dense_vec,\n",
        "                \"text-sparse\": models.SparseVector(\n",
        "                    indices=sparse_vec.indices.tolist(),\n",
        "                    values=sparse_vec.values.tolist()\n",
        "                ),\n",
        "            },\n",
        "        )\n",
        "    )\n",
        "\n",
        "client.upsert(\n",
        "    collection_name=collection_name,\n",
        "    points=points_to_upsert,\n",
        "    wait=True\n",
        ")\n",
        "print(f\"Successfully embedded and upserted all {len(docs)} documents.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "cFWTWzayVFcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 3: Initialize the Re-Ranker**\n",
        "\n",
        "Now, we introduce our new component. We will load a powerful Cross-Encoder model from the `sentence-transformers` library. This model is specifically trained to predict the relevance score between a query and a document."
      ],
      "metadata": {
        "id": "cLVm2UwfVFcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Task: Initialize the CrossEncoder model.\n",
        "# HINT: Use the model name 'BAAI/bge-reranker-base'.\n",
        "cross_encoder = # ... complete this line\n",
        "\n",
        "\n",
        "print(\"Re-ranker model initialized successfully.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xLzI3zOuVFcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 4: Build the RAG Chain with Re-Ranking**\n",
        "\n",
        "This is the core of our upgrade. We will create a new retrieval function that first uses our hybrid search to get a broad set of candidate documents and then uses our new `cross_encoder` to re-rank them for precision."
      ],
      "metadata": {
        "id": "4oJBl3oBVFcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize the Groq LLM (as provided by instructor)\n",
        "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "# --- Helper function to visualize the context ---\n",
        "def pretty_print_docs(docs, title):\n",
        "    print(f\"--- {title} ---\")\n",
        "    print(f\"Found {len(docs)} documents.\\n\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get('source', 'Unknown Source'); page = doc.metadata.get('page', 'Unknown Page')\n",
        "        score = f\" | Score: {doc.score:.4f}\" if hasattr(doc, 'score') and doc.score is not None else \"\"\n",
        "        print(f\"  [{i+1}] Source: {source} (Page: {page}){score}\")\n",
        "        print(f\"      Content: '{doc.page_content[:120]}...'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- New Retrieval Function with Re-Ranking ---\n",
        "def rerank_and_retrieve(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs and visualizes a two-stage retrieval process.\n",
        "    \"\"\"\n",
        "    # === Stage 1: Initial Retrieval (Casting a Wider Net) ===\n",
        "    print(\"--- 1. Performing Initial Hybrid Search ---\")\n",
        "    top_k_retrieval = 10\n",
        "\n",
        "    # YOUR CODE HERE (Part 1)\n",
        "    # Task: Create the dense and sparse vectors for the query.\n",
        "    dense_query_vec = # ... complete this line\n",
        "    sparse_query_vec = # ... complete this line\n",
        "\n",
        "    # YOUR CODE HERE (Part 2)\n",
        "    # Task: Perform the dense and sparse searches using the client.search() method.\n",
        "    # Make sure to use top_k_retrieval for the limit.\n",
        "    dense_results = # ... complete this line\n",
        "    sparse_results = # ... complete this line\n",
        "\n",
        "    # The fusion logic is provided for you\n",
        "    seen_ids = set()\n",
        "    candidate_docs_lc, candidate_docs_text = [], []\n",
        "    all_results = dense_results + sparse_results\n",
        "    for result in all_results:\n",
        "        if result.id not in seen_ids:\n",
        "            doc = Document(page_content=result.payload.get('text', ''), metadata={k: v for k, v in result.payload.items() if k != 'text'})\n",
        "            candidate_docs_lc.append(doc)\n",
        "            candidate_docs_text.append(result.payload['text'])\n",
        "            seen_ids.add(result.id)\n",
        "    pretty_print_docs(candidate_docs_lc, \"Initial Hybrid Search Candidates\")\n",
        "\n",
        "    # === Stage 2: Re-Ranking for Precision ===\n",
        "    print(\"\\n--- 2. Applying Cross-Encoder to Re-Rank for Precision ---\")\n",
        "    # YOUR CODE HERE (Part 3)\n",
        "    # Task: Create the pairs of [query, document_text] for the re-ranker.\n",
        "    rerank_pairs = # ... complete this line\n",
        "\n",
        "    # YOUR CODE HERE (Part 4)\n",
        "    # Task: Get the relevance scores by calling cross_encoder.predict().\n",
        "    rerank_scores = # ... complete this line\n",
        "\n",
        "    # The sorting and selection logic is provided for you\n",
        "    doc_with_scores = list(zip(candidate_docs_lc, rerank_scores))\n",
        "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_k_rerank = 3\n",
        "    final_docs = [doc[0] for doc in sorted_docs[:top_k_rerank]]\n",
        "    pretty_print_docs(final_docs, f\"Top {top_k_rerank} Re-Ranked Documents\")\n",
        "\n",
        "    final_context = \"\\n---\\n\".join([doc.page_content for doc in final_docs])\n",
        "    return final_context\n",
        "\n",
        "\n",
        "# --- Build the RAG Chain (This part is provided for you) ---\n",
        "prompt_template = \"Answer the question based only on the following context:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "rag_chain_with_reranker = (\n",
        "    {\"context\": RunnablePassthrough() | (lambda q: rerank_and_retrieve(q)), \"question\": RunnablePassthrough()}\n",
        "    | prompt | llm | StrOutputParser()\n",
        ")\n",
        "print(\"\\nRAG chain with Re-Ranker initialized successfully.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lfh4MjGzVFcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Step 5: Validate the Solution**\n",
        "\n",
        "It's time to test our new, high-precision system. We will run the exact same query that failed in Module 2 and see if the re-ranker fixed the problem."
      ],
      "metadata": {
        "id": "KI_iQajrVFcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This part is provided for you\n",
        "\n",
        "# The query that failed in Module 2 due to a recall failure\n",
        "query = \"What was the exact value for \\\"Tax withholding related to common stock from stock plans\\\" for the period ending April 27, 2025?\"\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "answer = rag_chain_with_reranker.invoke(query)\n",
        "print(f\"Answer: {answer}\\n\")\n",
        "print(\"-\" * 50)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "o012aqlgVFcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3 Conclusion & Analysis\n",
        "\n",
        "After running our query, `\"What was the exact value for 'Tax withholding related to common stock from stock plans' for the period ending April 27, 2025?\"`, you should see that the system now provides the **correct** answer: **$1,532 million**.\n",
        "\n",
        "This is a critical result, especially because the Module 2 pipeline (with `k=4`) failed on this exact query. Let's analyze why the Module 3 pipeline succeeded where the previous one failed.\n",
        "\n",
        "### Why it Worked: A Two-Stage Solution to a Two-Part Problem\n",
        "\n",
        "Our success is not just because of the re-ranker alone. It's because we implemented a more robust, two-stage retrieval strategy that is a common pattern in production-grade systems.\n",
        "\n",
        "**1. Part One: Solving the Recall Failure (The Wider Net)**\n",
        "First, we addressed the **Recall Failure** we saw in Module 2. Our original system with `k=4` was too \"narrow\" and failed to retrieve the correct document chunk from the dense table on page 6. In this module, the *first* part of our solution was to increase the initial retrieval size to `k=10`. This \"casts a wider net,\" ensuring that even if the correct chunk has a low initial score, it's very likely to be included in our list of candidates. This step is all about maximizing **recall**—making sure the right answer is found in the first place.\n",
        "\n",
        "**2. Part Two: Solving the Precision Failure (The Intelligent Filter)**\n",
        "Casting a wider net creates a new, more subtle problem: our candidate list is now much **noisier**. If we passed this entire messy list to the LLM, we would be hoping for a \"fragile success\" and risking a wrong answer.\n",
        "\n",
        "This is where the **re-ranker** proves its value. It acts as an intelligent, high-precision filter. The `bge-reranker-base` Cross-Encoder analyzed this noisy list of candidates and performed a deep comparison of the query's specific intent. It recognized that the specific table row from page 6 was a much more precise match than any other chunk and promoted it to the top of the list. By passing only the top 3 re-ranked chunks, we provided clean, unambiguous context to the LLM, allowing it to easily extract the correct number.\n",
        "\n",
        "\n",
        "### Key Takeaway: Engineering a Robust RAG System\n",
        "\n",
        "The key lesson is that building a trustworthy RAG system requires engineering a pipeline that solves for both recall and precision.\n",
        "\n",
        "* **The Retriever (`k=10`)** is our fast, high-recall component. Its job is to cast a wide net and make sure the answer is on the table.\n",
        "* **The Re-ranker** is our slow, high-precision component. Its job is to inspect everything the retriever found and identify the single best piece of evidence.\n",
        "\n",
        "This two-stage process is the foundation of building systems that are not just powerful, but also **reliable and trustworthy**. We are no longer just hoping the LLM can figure it out; we are engineering the best possible conditions for it to succeed every time.\n",
        "\n",
        "**Next Up:** Now that we have built a powerful and precise RAG pipeline, how do we prove it? In **Module 4**, we will learn how to **evaluate our system quantitatively** using the RAGAS framework to measure its performance on key metrics like faithfulness and relevancy."
      ],
      "metadata": {
        "id": "yCNend8LVFcE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}