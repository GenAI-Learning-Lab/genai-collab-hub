{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azzlUMhNQxgP"
   },
   "source": [
    "# **AI-Powered Financial Analysis System - Project Prerequisites**\n",
    "\n",
    "This document outlines the necessary setup to complete all modules of the RAG learning roadmap. Please configure these items before starting Module 1.\n",
    "\n",
    "### **Required API Keys**\n",
    "\n",
    "You will need to create free accounts for the following services and obtain API keys. We will load these into our Google Colab environment using the secrets manager.\n",
    "\n",
    "1.  **Groq API Key**\n",
    "    * **Purpose:** Provides access to high-speed LLM inference.\n",
    "    * **Get it here:** [https://console.groq.com/keys](https://console.groq.com/keys)\n",
    "\n",
    "2.  **Hugging Face User Access Token**\n",
    "    * **Purpose:** Allows us to download models and use the Hugging Face ecosystem. A `read` role is sufficient.\n",
    "    * **Get it here:** [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "### **Required Files & Environment**\n",
    "\n",
    "1.  **Environment:** Google Colab.\n",
    "2.  **Source Document:** The **NVIDIA Q1 FY26 Earnings Report** PDF.\n",
    "    * **Action:** Download the press release PDF from the official NVIDIA news site: [NVIDIA Q1 FY2026 Financial Results](https://investor.nvidia.com/financial-info/quarterly-results/default.aspx).\n",
    "\n",
    "    DIRECT PDF Download LINK -> https://s201.q4cdn.com/141608511/files/doc_financials/2026/q1/b6df1c5c-5cb6-4a41-9d28-dd1bcd34cc26.pdf\n",
    "    * You will need to upload this file (`.pdf`) to your Colab session at the start of each module.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **Module 1: The Foundation - Basic RAG**\n",
    "\n",
    "### **Objective**\n",
    "In this module, we will build the simplest possible Retrieval-Augmented Generation (RAG) pipeline. This serves as our **performance baseline**. Our goal is to understand the fundamental workflow of a RAG system and to observe its inherent limitations when applied to a complex financial document.\n",
    "\n",
    "### **Core Concept: The Classic RAG Pipeline**\n",
    "We will implement the foundational \"Load -> Split -> Embed -> Store -> Retrieve -> Generate\" workflow.\n",
    "* **Load:** Ingest the NVIDIA financial report PDF.\n",
    "* **Split:** Break the document into smaller, manageable chunks.\n",
    "* **Embed:** Convert each chunk into a numerical representation (vector).\n",
    "* **Store:** Save these vectors in a specialized database for efficient searching.\n",
    "* **Retrieve:** Given a user's query, find the most relevant chunks from the database.\n",
    "* **Generate:** Pass the retrieved chunks and the original query to a Large Language Model (LLM) to generate a final answer.\n",
    "\n",
    "### **Business Impact**\n",
    "By testing this simple system against our stakeholder queries, we will see where it succeeds and, more importantly, where it fails. This provides a clear, data-driven justification for the more advanced techniques we will implement in later modules to improve accuracy and reliability.\n",
    "\n",
    "---\n",
    "### **Step 1: Install Dependencies**\n",
    "First, we install all the necessary open-source libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m167.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m798.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-community langchain-groq qdrant-client sentence-transformers pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdNMb8GXQxgT"
   },
   "source": [
    "---\n",
    "### **Step 2: Set Up API Keys**\n",
    "We need to configure our API keys for Groq (LLM) and Hugging Face (embeddings). Please add your keys to the Colab secrets manager (key icon on the left) with the names `GROQ_API_KEY` and `HF_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set up the API keys\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
    "\n",
    "print(\"API keys set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTTD5y5_QxgT"
   },
   "source": [
    "---\n",
    "### **Step 3: Load and Split the Document**\n",
    "Here, we perform the first two stages of our RAG pipeline: **Load** and **Split**. We'll load the NVIDIA PDF and use a `RecursiveCharacterTextSplitter` to create text chunks that are small enough to be processed effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and split the document into 191 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF\n",
    "# Make sure you have uploaded the NVIDIA Q1 FY25 earnings PDF to your Colab session\n",
    "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Successfully loaded and split the document into {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhhlbSn1QxgT"
   },
   "source": [
    "---\n",
    "### **Step 4: Embed and Store in Vector Database**\n",
    "Now for the **Embed** and **Store** stages. We will use the powerful `bge-m3` model from Hugging Face to create vector embeddings for our chunks. These vectors will be stored in a **Qdrant** vector database running entirely in memory, which is perfect for our notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4-2966727358.py:8: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceBgeEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caa885b7bc64675a09d96c7c40f1cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbe6539486e4b589dec979a9d52b908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e683d3066549bf83f2666b09440046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8746d45140d4ba79e036873e01a1968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dc7e9de9954412aa474f407d55246b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c65433669c4e2da32d4d44621969c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f84e252efe4223a35b61cd86bfbdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf83b2a2e9a4e3db23ace3e8956dd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e7d88090da4c4899a329b242c3ae7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c4bdd466644d48b2a4c75cc318bea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6e0e654c3f4a12a054bc042da03d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2744180eacd4aee9bce16238532e6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully embedded documents and stored them in Qdrant.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# Initialize our embedding model\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"} # Use CPU for embedding, can be changed to \"cuda\" if GPU is available\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "# Store the embedded documents in a Qdrant vector store\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embedding_model,\n",
    "    location=\":memory:\",  # Create an in-memory Qdrant instance\n",
    "    collection_name=\"nvidia_earnings\",\n",
    ")\n",
    "\n",
    "print(\"Successfully embedded documents and stored them in Qdrant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBkeQ01kQxgU"
   },
   "source": [
    "---\n",
    "### **Step 5: Initialize the RAG Chain**\n",
    "This is where we tie everything together. We'll set up our Groq LLM, create a retriever to fetch documents from Qdrant, and build the final RAG chain using LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM7s5VaeQxgU"
   },
   "source": [
    "---\n",
    "### **Step 6: Test the Baseline System**\n",
    "It's time to evaluate our baseline system. We will ask the four critical stakeholder queries and see how it performs. This will reveal the strengths and weaknesses of a simple semantic-search-based RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What were the drivers of the year-over-year increase in Compute & Networking segment operating income for Q1 FY26?\n",
      "\n",
      "Answer: According to the document, the year-over-year increase in Compute & Networking segment operating income in the first quarter of fiscal year 2026 was driven by growth in revenue, partially offset by a $4.5 billion charge associated with H20 excess inventory and purchase obligations.\n",
      "\n",
      "--------------------------------------------------\n",
      "Query: What was the Research and development expense for the three months ended April 27, 2025?\n",
      "\n",
      "Answer: According to the provided context, the Research and development expenses for the three months ended April 27, 2025, was $3,989 million.\n",
      "\n",
      "--------------------------------------------------\n",
      "Query: What was the total charge incurred in Q1 FY2026 related to H20 excess inventory and purchase obligations?\n",
      "\n",
      "Answer: According to the document, the total charge incurred in Q1 FY2026 related to H20 excess inventory and purchase obligations was $4.5 billion. This charge was mentioned in two documents:\n",
      "\n",
      "1. In the document with page label '24', it was stated that: \"As a result of these new requirements, we incurred a $4.5 billion charge in the first quarter of fiscal year2026 associated with H20 excess inventory and purchase obligations...\"\n",
      "2. In the document with page label '13', it was stated that: \"The $1.9 billion inventory provision for H20 product inventory is part of the overall $4.5 billion charge associated with H20 product excess inventory and purchase obligations...\" and also in the document with page label '14', it was stated that: \"The $2.6 billion excess inventory purchase obligation charge for H20 product orders is part of the overall $4.5 billion charge associated with H20 product excess inventory and purchase obligations...\" \n",
      "\n",
      "So, the total charge incurred in Q1 FY2026 related to H20 excess inventory and purchase obligations was $4.5 billion.\n",
      "\n",
      "--------------------------------------------------\n",
      "Query: How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\n",
      "\n",
      "Answer: The information is not present in the provided context. The context provides details on various financial aspects of NVIDIA Corporation, including property and equipment, income taxes, purchase commitments, and cash flows, but it does not mention share repurchases for the first quarter of fiscal year 2026.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Our stakeholder-driven test queries, derived from the Q1 FY26 report\n",
    "queries = [\n",
    "    # 1. For the Financial Analyst (requires nuanced understanding)\n",
    "    \"What were the drivers of the year-over-year increase in Compute & Networking segment operating income for Q1 FY26?\",\n",
    "\n",
    "    # 2. For the Portfolio Manager (requires table data extraction)\n",
    "    \"What was the Research and development expense for the three months ended April 27, 2025?\",\n",
    "\n",
    "    # 3. For the CIO/Risk Officer (requires specific fact retrieval)\n",
    "    \"What was the total charge incurred in Q1 FY2026 related to H20 excess inventory and purchase obligations?\",\n",
    "\n",
    "    # 4. For the Retail Investor (requires specific fact retrieval)\n",
    "    \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
    "]\n",
    "\n",
    "# Run the queries through our RAG chain\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA8bV6JUQxgV"
   },
   "source": [
    "## **Module 1 Conclusion: Analyzing the Baseline’s Surprising Successes and Critical Failures**\n",
    "\n",
    "The real-world performance of our baseline system is fascinating and provides us with invaluable insights. We saw surprising successes alongside a critical failure that perfectly highlights the limitations of a naive RAG approach.\n",
    "\n",
    "---\n",
    "\n",
    "### Analysis of Results\n",
    "\n",
    "- **The Successes (Analyst, Portfolio Manager & CIO Queries):**\n",
    "\n",
    "  - **Analyst & CIO Queries**: The system performed exceptionally well, correctly answering the nuanced question about operating income drivers and the specific query about the $4.5 billion charge. This indicates that when the answer is contained in a clear, semantically distinct sentence, the basic retriever works effectively.\n",
    "\n",
    "  - **The Brittle Success (Portfolio Manager’s R&D Query)**: The system’s ability to correctly pull the R&D expense ($3,989 million) from a table is a surprising success. However, this should be viewed as **unreliable and likely coincidental**. The simple `PyPDFLoader` does not truly understand table structures; it merely extracted the text in an order that, by chance, kept the \"Research and development\" string close enough to its value for the LLM to connect them. This approach is fragile and would fail with more complex tables or comparative queries.\n",
    "\n",
    "- **The Critical Failure (Retail Investor’s Share Repurchase Query):**\n",
    "\n",
    "  - This is our most important learning point. The system confidently stated that the document \"**does not contain information about share repurchases**\", which is factually incorrect. The report explicitly states on page 28:  \n",
    "    *\"We repurchased 126 million… shares… for $14.5 billion… during the first quarter of fiscal years 2026\".*\n",
    "\n",
    "  - **Diagnosis**: This is a classic **Retrieval Failure**. The data was loaded and chunked, but the retriever failed to identify the chunk containing the answer as being semantically relevant to the query \"How much did NVIDIA spend on share repurchases…\". Because the retriever passed an empty or irrelevant context to the LLM, the LLM correctly (but misleadingly) reported that the information was not available.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "Our baseline system is **unpredictable and not production-ready**. Its success feels more like luck than robust design. The silent retrieval failure is particularly dangerous, as it can mislead a user into believing information is absent when it is, in fact, present.  \n",
    "This single failure demonstrates why we cannot deploy such a simple system.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Up:\n",
    "\n",
    "The retrieval failure on the share repurchase query gives us a clear mission.  \n",
    "We need to make our retriever more robust. **In Module 2**, we will directly address this by implementing **Hybrid Search**, which combines semantic search with keyword matching. This technique is specifically designed to prevent failures on queries containing precise terms like \"share repurchases.\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
