{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# **AI-Powered Financial Analysis System - Project Prerequisites**\n",
        "\n",
        "This document outlines the necessary setup to complete all modules of the RAG learning roadmap. Please configure these items before starting Module 1.\n",
        "\n",
        "### **Required API Keys**\n",
        "\n",
        "You will need to create free accounts for the following services and obtain API keys. We will load these into our Google Colab environment using the secrets manager.\n",
        "\n",
        "1.  **Groq API Key**\n",
        "    * **Purpose:** Provides access to high-speed LLM inference.\n",
        "    * **Get it here:** [https://console.groq.com/keys](https://console.groq.com/keys)\n",
        "\n",
        "2.  **Hugging Face User Access Token**\n",
        "    * **Purpose:** Allows us to download models and use the Hugging Face ecosystem. A `read` role is sufficient.\n",
        "    * **Get it here:** [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "### **Required Files & Environment**\n",
        "\n",
        "1.  **Environment:** Google Colab.\n",
        "2.  **Source Document:** The **NVIDIA Q1 FY26 Earnings Report** PDF.\n",
        "    * **Action:** Download the press release PDF from the official NVIDIA news site: [NVIDIA Q1 FY2026 Financial Results](https://investor.nvidia.com/financial-info/quarterly-results/default.aspx).\n",
        "\n",
        "    DIRECT PDF Download LINK -> https://s201.q4cdn.com/141608511/files/doc_financials/2026/q1/b6df1c5c-5cb6-4a41-9d28-dd1bcd34cc26.pdf\n",
        "    * You will need to upload this file (`.pdf`) to your Colab session at the start of each module.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Module 1: The Foundation - Basic RAG**\n",
        "\n",
        "### **Objective**\n",
        "In this module, we will build the simplest possible Retrieval-Augmented Generation (RAG) pipeline. This serves as our **performance baseline**. Our goal is to understand the fundamental workflow of a RAG system and to observe its inherent limitations when applied to a complex financial document.\n",
        "\n",
        "### **Core Concept: The Classic RAG Pipeline**\n",
        "We will implement the foundational \"Load -> Split -> Embed -> Store -> Retrieve -> Generate\" workflow.\n",
        "* **Load:** Ingest the NVIDIA financial report PDF.\n",
        "* **Split:** Break the document into smaller, manageable chunks.\n",
        "* **Embed:** Convert each chunk into a numerical representation (vector).\n",
        "* **Store:** Save these vectors in a specialized database for efficient searching.\n",
        "* **Retrieve:** Given a user's query, find the most relevant chunks from the database.\n",
        "* **Generate:** Pass the retrieved chunks and the original query to a Large Language Model (LLM) to generate a final answer.\n",
        "\n",
        "### **Business Impact**\n",
        "By testing this simple system against our stakeholder queries, we will see where it succeeds and, more importantly, where it fails. This provides a clear, data-driven justification for the more advanced techniques we will implement in later modules to improve accuracy and reliability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "### **Step 1: Install Dependencies**\n",
        "First, we install all the necessary open-source libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-groq qdrant-client langchain_qdrant langchain_huggingface pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### **Step 2: Set Up API Keys**\n",
        "We need to configure our API keys for Groq (LLM) and Hugging Face (embeddings). Please add your keys to the Colab secrets manager (key icon on the left) with the names `GROQ_API_KEY` and `HF_TOKEN`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up the API keys\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "print(\"API keys set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### **Step 3: Load and Split the Document**\n",
        "Here, we perform the first two stages of our RAG pipeline: **Load** and **Split**. We'll load the NVIDIA PDF and use a `RecursiveCharacterTextSplitter` to create text chunks that are small enough to be processed effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load the PDF\n",
        "# Make sure you have uploaded the NVIDIA Q1 FY25 earnings PDF to your Colab session\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Successfully loaded and split the document into {len(docs)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### **Step 4: Embed and Store in Vector Database**\n",
        "Now for the **Embed** and **Store** stages. We will use the powerful `bge-m3` model from Hugging Face to create vector embeddings for our chunks. These vectors will be stored in a **Qdrant** vector database running entirely in memory, which is perfect for our notebook environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 1. Initialize our embedding model\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs,\n",
        ")\n",
        "\n",
        "collection_name = \"nvidia_earnings\"\n",
        "\n",
        "# 2. Create a Qdrant client for an in-memory database\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# 3. Create the collection in Qdrant if it does not exist\n",
        "if not client.collection_exists(collection_name=collection_name):\n",
        "    # The BAAI/bge-m3 model has a vector size of 1024\n",
        "    client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"Successfully created collection '{collection_name}'.\")\n",
        "else:\n",
        "    print(f\"Collection '{collection_name}' already exists.\")\n",
        "\n",
        "# 4. Instantiate the QdrantVectorStore with the existing client and collection\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=embedding_model,\n",
        ")\n",
        "\n",
        "# 5. Add the documents to the vector store\n",
        "vectorstore.add_documents(docs)\n",
        "print(\"Successfully embedded and stored documents in Qdrant.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Vector databases like Qdrant are highly optimized to perform dot product calculations very, very quickly. By normalizing the embeddings on the client-side (inside the HuggingFaceEmbeddings class), we allow Qdrant to use its fastest possible calculation (dot product) to give us the results of a cosine similarity search. It's a standard and important performance optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### **Step 5: Initialize the RAG Chain**\n",
        "This is where we tie everything together. We'll set up our Groq LLM, create a retriever to fetch documents from Qdrant, and build the final RAG chain using LangChain Expression Language (LCEL).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize the Groq LLM\n",
        "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "# Create the retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Create the prompt template\n",
        "prompt_template = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Build the RAG chain using LCEL\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Add a step to print the retrieved documents\n",
        "def print_retrieved_docs(docs):\n",
        "    print(\"Retrieved Documents:\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        print(f\"--- Document {i+1} ---\")\n",
        "        print(doc.page_content)\n",
        "        print(f\"Source: {doc.metadata.get('source')}, Page: {doc.metadata.get('page')}\")\n",
        "        print(\"-\" * 20)\n",
        "    return docs\n",
        "\n",
        "rag_chain_with_printing = (\n",
        "    {\"context\": retriever | print_retrieved_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain initialized successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### **Step 6: Test the Baseline System**\n",
        "It's time to evaluate our baseline system. We will ask the four critical stakeholder queries and see how it performs. This will reveal the strengths and weaknesses of a simple semantic-search-based RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our stakeholder-driven test queries, derived from the Q1 FY26 report\n",
        "queries = [\n",
        "    # 1. For the Financial Analyst (requires nuanced understanding)\n",
        "    #\"What were the drivers of the year-over-year increase in Compute & Networking segment operating income for Q1 FY26?\",\n",
        "\n",
        "    # 2. For the Portfolio Manager (requires table data extraction)\n",
        "    #\"What was the Research and development expense for the three months ended April 27, 2025?\",\n",
        "\n",
        "    # 3. For the CIO/Risk Officer (requires specific fact retrieval)\n",
        "    #\"What was the total charge incurred in Q1 FY2026 related to H20 excess inventory and purchase obligations?\",\n",
        "\n",
        "    # 4. For the Retail Investor (requires specific fact retrieval)\n",
        "    \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
        "]\n",
        "\n",
        "# Run the queries through our RAG chain\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    answer = rag_chain_with_printing.invoke(query) # Use the chain with printing\n",
        "    print(f\"Answer: {answer}\\n\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## **Module 1 Conclusion: Analyzing the Baseline's Surprising Successes and Critical Failures**\n",
        "\n",
        "The real-world performance of our baseline system is fascinating and provides us with invaluable insights. We saw surprising successes alongside a critical failure that perfectly highlights the limitations of a naive RAG approach.\n",
        "\n",
        "---\n",
        "\n",
        "### Analysis of Results\n",
        "\n",
        "- **The Successes (Analyst, Portfolio Manager & CIO Queries):**\n",
        "\n",
        "  - **Analyst & CIO Queries**: The system performed exceptionally well, correctly answering the nuanced question about operating income drivers and the specific query about the $4.5 billion charge. This indicates that when the answer is contained in a clear, semantically distinct sentence, the basic retriever works effectively.\n",
        "\n",
        "  - **The Brittle Success (Portfolio Manager's R&D Query)**: The system's ability to correctly pull the R&D expense ($3,989 million) from a table is a surprising success. However, this should be viewed as **unreliable and likely coincidental**. The simple `PyPDFLoader` does not truly understand table structures; it merely extracted the text in an order that, by chance, kept the \"Research and development\" string close enough to its value for the LLM to connect them. This approach is fragile and would fail with more complex tables or comparative queries.\n",
        "\n",
        "- **The Critical Failure (Retail Investor's Share Repurchase Query):**\n",
        "\n",
        "  - This is our most important learning point. The system confidently stated that the document \"**does not contain information about share repurchases**\", which is factually incorrect. The report explicitly states on page 28:  \n",
        "    *\"We repurchased 126 million… shares… for $14.5 billion… during the first quarter of fiscal years 2026\".*\n",
        "\n",
        "  - **Diagnosis**: This is a classic **Retrieval Failure**. The data was loaded and chunked, but the retriever failed to identify the chunk containing the answer as being semantically relevant to the query \"How much did NVIDIA spend on share repurchases…\".\n",
        "\n",
        "  **The Reason: Why the Retriever Failed**\n",
        "\n",
        "  The retriever prioritized chunks with high general semantic relevance over the chunk with the specific factual answer. The top 4 retrieved documents were all about NVIDIA's Q1 FY26 finances but lacked the keywords \"share repurchases.\" The correct chunk, which was part of a table on page 6, was ranked lower and thus missed by the k=4 cutoff.\n",
        "\n",
        "  **While it is tempting to simply increase k to 10 to get the answer**, this is not a robust solution for a production system.\n",
        "\n",
        "  - It's Inefficient: Retrieving more documents significantly increases cost and latency (slower response time) for every query.\n",
        "\n",
        "  - It Can Reduce Accuracy: LLMs suffer from a \"lost in the middle\" problem. Burying the single correct document among many irrelevant ones increases the chance the LLM will ignore it, potentially leading to more incorrect answers.\n",
        "\n",
        "  - It Doesn't Fix the Root Cause: A production system cannot rely on luck. The core issue is an imprecise retriever. The goal is to improve retrieval quality to rank the best documents at the top, not just to retrieve more documents and hope for the best.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "This baseline RAG, while a useful starting point, is too unpredictable and brittle for production deployment. Its successes feel coincidental, while its failures are critical. The silent retrieval failure—confidently reporting that **information is absent when it's present—is a dangerous flaw that erodes user trust and demonstrates the system's fundamental lack of robustness.**\n",
        "\n",
        "---\n",
        "\n",
        "### Next Up:\n",
        "\n",
        "The retrieval failure on the share repurchase query gives us a clear mission.  \n",
        "We need to make our retriever more robust. **In Module 2**, we will directly address this by implementing **Hybrid Search**, which combines semantic search with keyword matching. This technique is specifically designed to prevent failures on queries containing precise terms like \"share repurchases.\"\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
