{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Module 1: The Basic RAG**\n",
        "\n",
        "**Objective:**\n",
        "In this module, we will build the simplest possible Retrieval-Augmented Generation (RAG) pipeline. This serves as our **performance baseline**. Our goal is to understand the fundamental workflow of a RAG system and to observe its inherent limitations when applied to a complex financial document.\n",
        "\n",
        "**Core Concept: The Classic RAG Pipeline**\n",
        "We will implement the foundational \"Load -\\> Split -\\> Embed -\\> Store -\\> Retrieve -\\> Generate\" workflow to build our AI Financial Analyst.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this module, you will be able to:\n",
        "\n",
        "  * **Understand the end-to-end workflow** of a fundamental RAG system.\n",
        "  * **Implement the six core stages:** Load, Split, Embed, Store, Retrieve, and Generate.\n",
        "  * **Use LangChain** to build a pipeline connecting a vector database (Qdrant) to a powerful LLM (Llama 4).\n",
        "  * **Perform semantic search** to retrieve document chunks based on conceptual meaning.\n",
        "  * **Analyze the output** to identify the limitations and critical failure points of a basic RAG approach.\n",
        "\n",
        "-----\n",
        "\n",
        "#### **Step 1: Install Dependencies**\n",
        "\n",
        "First, we install all the necessary open-source libraries."
      ],
      "metadata": {
        "id": "Sitqg1rI0v8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community langchain-groq qdrant-client sentence-transformers pypdf"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Mipw1Pz40v8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "#### **Step 2: Set Up API Key & Document Loading**\n",
        "\n",
        "This step configures our Groq API key and loads the NVIDIA PDF document, splitting it into manageable chunks."
      ],
      "metadata": {
        "id": "_LxkpT_q0v8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up the API key from Colab secrets\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# --- Document Loading and Splitting ---\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Make sure you have uploaded the NVIDIA Q1 FY26 earnings PDF\n",
        "pdf_path = \"./NVIDIA-Q1-FY26-Financial-Results.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Document loaded and split into {len(docs)} chunks.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Sq75DhJ50v8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "#### **Step 3: Embed and Store in Vector Database**\n",
        "\n",
        "Now, we will create vector embeddings for our text chunks and store them in our Qdrant vector database."
      ],
      "metadata": {
        "id": "EvaRw1bZ0v8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "# YOUR CODE HERE (Part 1)\n",
        "# Initialize the HuggingFaceBgeEmbeddings model.\n",
        "# Use the model name \"BAAI/bge-m3\".\n",
        "# Set the model_kwargs to use the \"cpu\" and encode_kwargs to normalize embeddings.\n",
        "embedding_model = ...\n",
        "\n",
        "print(\"Embedding model initialized.\")\n",
        "\n",
        "# YOUR CODE HERE (Part 2)\n",
        "# Use the Qdrant.from_documents() class method to create the vector store.\n",
        "# This single command will create the embeddings for all 'docs' and store them.\n",
        "# It needs the 'docs', the 'embedding_model', an in-memory 'location', and a 'collection_name'.\n",
        "vectorstore = ...\n",
        "\n",
        "print(\"Successfully embedded documents and stored them in Qdrant.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UTRu8X4d0v8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "#### **Step 4: Initialize the RAG Chain**\n",
        "\n",
        "This is where we tie everything together using LangChain Expression Language (LCEL) to create our final pipeline."
      ],
      "metadata": {
        "id": "Iru5drT70v8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# YOUR CODE HERE (Part 1)\n",
        "# Initialize the ChatGroq LLM with the 'llama3-8b-8192' model and a temperature of 0.\n",
        "llm = ...\n",
        "\n",
        "# Initialize a retriever from the vectorstore you created in the previous step.\n",
        "# HINT: Use the .as_retriever() method on your vectorstore object.\n",
        "retriever = ...\n",
        "\n",
        "# Create the prompt template string.\n",
        "# It should instruct the LLM to answer based *only* on the {context}, and it needs a {question} placeholder.\n",
        "prompt_template_string = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template_string)\n",
        "\n",
        "\n",
        "# YOUR CODE HERE (Part 2)\n",
        "# Build the RAG chain using LCEL and the pipe operator (|).\n",
        "# The first part should be a dictionary that prepares the context and question.\n",
        "# Then, pipe it to the prompt, then the llm, and finally to the StrOutputParser.\n",
        "rag_chain = (\n",
        "    ...\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain initialized successfully.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xek3k1N40v80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "#### **Step 5: Test the Baseline System**\n",
        "\n",
        "Now we run our stakeholder queries through the system to see how our baseline performs."
      ],
      "metadata": {
        "id": "bJrL6a3X0v80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our stakeholder-driven test queries\n",
        "queries = [\n",
        "    \"What were the drivers of the year-over-year increase in Compute & Networking segment operating income for Q1 FY26?\",\n",
        "    \"What was the Research and development expense for the three months ended April 27, 2025?\",\n",
        "    \"What was the total charge incurred in Q1 FY2026 related to H20 excess inventory and purchase obligations?\",\n",
        "    \"How much did NVIDIA spend on share repurchases in the first quarter of fiscal year 2026?\"\n",
        "]\n",
        "\n",
        "# Run the queries through our RAG chain\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    answer = rag_chain.invoke(query)\n",
        "    print(f\"Answer: {answer}\\n\")\n",
        "    print(\"-\" * 50)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Ic12Nf_k0v80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "#### **Module 1 Conclusion: Analyzing the Baseline Performance**\n",
        "\n",
        "*After running the code, analyze the output. Did it answer all questions correctly? Where did it fail? The most likely failure will be on the \"share repurchases\" query. This is our key learning: basic semantic search is powerful but not always reliable, which sets the stage for our next module.*"
      ],
      "metadata": {
        "id": "5Vfqebit0v80"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}